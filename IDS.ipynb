{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Engineer-Ayesha-Shafique/Intrusion-Detection-System--UNSW_NB15/blob/main/IDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XExu7qDXt-Ol"
      },
      "source": [
        "# UNSW-NB15: Data Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdEtZ6YBuIuj"
      },
      "source": [
        "## Source/Useful Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GSJZmgbuUNN"
      },
      "source": [
        "Get the data from: https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/\n",
        "\n",
        "Refer: http://dx.doi.org/10.1080/19393555.2015.1125974\n",
        "\n",
        "\n",
        "This dataset has been created by: Nour Moustafa, IEEE student Member, Jill Slay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9hQKKJQy6Se"
      },
      "source": [
        "## What is IDS (Intrusion Detection System)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgahjiyXJllR"
      },
      "source": [
        "Intrusion Detection Systems (IDS) are precisely present to prevent attacks and infiltration to Networks, which might affect the organization. They monitor network traffic for suspicious activities and issue alert in case of issues.\n",
        "\n",
        "**Types if IDS:**\n",
        "> 1. *Signature-based intrusion detection*– In this kind incoming attacks are compared with pre-existing database of known attacks.\n",
        "> 2. *Anomaly-based intrusion detection*- It uses statistics to form a baseline usage of the networks at different time intervals. They were introduced to detect unknown attacks.\n",
        "\n",
        "Based on where they discover, they can be classified into:\n",
        "1. Network intrusion detection (NIDS)\n",
        "2. Host intrusion detection (HIDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCIAUYTQzNXL"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vknah70vLLPB"
      },
      "source": [
        "With the rise of Internet usage, it is very important to protect Networks.  The most common risk to a network’s security is an intrusion such as brute force, denial of service or even an infiltration from within a network. With the changing patterns in network behavior, it is necessary to switch to a dynamic approach to detect and prevent such intrusions.\n",
        "\n",
        "**Importance of this dataset:**\n",
        "\n",
        "Although there were few daatset available before this dataset for NIDS, but they were generated decades ago and do not provide realistic outputs. That's why this dataset had been created by Nour Moustafa to tackle existing problems like: unbalanced dataset, missing values etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41Rlj0SsvJs-"
      },
      "source": [
        "## Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5m0e4e5vPx5"
      },
      "source": [
        "This dataset has 4 CSV files of the data records and each CSV file contains attack and normal records.\n",
        "<table>\n",
        "<tr>\n",
        "<th> file name </th>\n",
        "<th> file name size</th>\n",
        "<th> number of records </th>\n",
        "<th> number of features </th>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td> UNSWNB15_1.csv </td>\n",
        "<td> 161.2 MB </td>\n",
        "<td> 700000 </td>\n",
        "<td> 49 </td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td> UNSWNB15_2.csv </td>\n",
        "<td> 157.6 MB </td>\n",
        "<td> 700000 </td>\n",
        "<td> 49 </td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td> UNSWNB15_3.csv </td>\n",
        "<td> 147.4 MB </td>\n",
        "<td> 700000 </td>\n",
        "<td> 49 </td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "<td> UNSWNB15_4.csv </td>\n",
        "<td> 91.3 MB </td>\n",
        "<td> 440044 </td>\n",
        "<td> 49 </td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N4GIGDlxQ5E"
      },
      "source": [
        "## Features in the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upBxlkRQxrvx"
      },
      "source": [
        "This dataset has 49 features.\n",
        "<br>\n",
        "There are 3 different datatypes:\n",
        "- Categorical: proto, state, service, attack_cat\n",
        "- Binary: is_sm_ips_ports, is_ftp_login\n",
        "- Numerical: Rest of the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oFD3hTTyxpE"
      },
      "source": [
        "## ML Problem Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPeudrf2MoII"
      },
      "source": [
        "*Binary classification of attack category*\n",
        "\n",
        "The dataset has \"label\" with 0 and 1 where 0 represents non-attack and 1 represent attack. So with the features available we will try to predict a given datapoint whether it belongs to attack or non-attack category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-OMBMVny0u6"
      },
      "source": [
        "## Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj58MEtaLN3F"
      },
      "source": [
        "1. Accuracy\n",
        "2. False Alarm Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEzmOzrvqhC2"
      },
      "source": [
        "# UNSW-NB15: Data cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z47K0yNEqyAN"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6ONB_pfCiB_"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # for array\n",
        "import pandas as pd  # for csv files and dataframe\n",
        "import matplotlib.pyplot as plt  # for plotting\n",
        "import seaborn as sns  # plotting\n",
        "from scipy import stats\n",
        "\n",
        "import pickle  # To load data int disk\n",
        "from prettytable import PrettyTable  # To print in tabular format\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
        "from sklearn.metrics import auc, f1_score, roc_curve\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPZ76vVwqsGD"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8FyMkxn7c-u"
      },
      "outputs": [],
      "source": [
        "# Creating a empty dict, where I will save all parameters required for test data transformation\n",
        "\n",
        "saved_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP2T1JbfEALU"
      },
      "outputs": [],
      "source": [
        "# Reading datasets\n",
        "dfs = []\n",
        "for i in range(1,5):\n",
        "    path = 'C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/data/UNSW-NB15_{}.csv'  # There are 4 input csv files\n",
        "    dfs.append(pd.read_csv(path.format(i), header = None))\n",
        "all_data = pd.concat(dfs).reset_index(drop=True)  # Concat all to a single df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIhtY1RMFI6V"
      },
      "outputs": [],
      "source": [
        "# This csv file contains names of all the features\n",
        "df_col = pd.read_csv('C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/data/NUSW-NB15_features.csv', encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4cRIiK3Furb"
      },
      "outputs": [],
      "source": [
        "# Making column names lower case, removing spaces\n",
        "df_col['Name'] = df_col['Name'].apply(lambda x: x.strip().replace(' ', '').lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA_VRzz3GQ5R"
      },
      "outputs": [],
      "source": [
        "# Renaming our dataframe with proper column names\n",
        "all_data.columns = df_col['Name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTbVl_fY801n"
      },
      "outputs": [],
      "source": [
        "# Saving useful info, later this will be used to transform raw test data\n",
        "saved_dict['columns'] = df_col['Name'][df_col['Name']!='label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1OsEYzyG5Mg"
      },
      "outputs": [],
      "source": [
        "del df_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "PhzQWEjjOh09",
        "outputId": "44ebab89-4be8-4108-c4df-e18b43c274a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "(2540047, 49)"
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "cioJQLS5GV0E",
        "outputId": "1bd31163-dab6-4bd9-b175-98b640801f8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Name       srcip  sport          dstip dsport proto state       dur  sbytes  \\\n0     59.166.0.0   1390  149.171.126.6     53   udp   CON  0.001055     132   \n1     59.166.0.0  33661  149.171.126.9   1024   udp   CON  0.036133     528   \n2     59.166.0.6   1464  149.171.126.7     53   udp   CON  0.001119     146   \n3     59.166.0.5   3593  149.171.126.5     53   udp   CON  0.001209     132   \n4     59.166.0.3  49664  149.171.126.0     53   udp   CON  0.001169     146   \n\nName  dbytes  sttl  ...  ct_ftp_cmd  ct_srv_src  ct_srv_dst ct_dst_ltm  \\\n0        164    31  ...           0           3           7          1   \n1        304    31  ...           0           2           4          2   \n2        178    31  ...           0          12           8          1   \n3        164    31  ...           0           6           9          1   \n4        178    31  ...           0           7           9          1   \n\nName  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n0              3                 1                 1               1   \n1              3                 1                 1               2   \n2              2                 2                 1               1   \n3              1                 1                 1               1   \n4              1                 1                 1               1   \n\nName  attack_cat  label  \n0            NaN      0  \n1            NaN      0  \n2            NaN      0  \n3            NaN      0  \n4            NaN      0  \n\n[5 rows x 49 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Name</th>\n      <th>srcip</th>\n      <th>sport</th>\n      <th>dstip</th>\n      <th>dsport</th>\n      <th>proto</th>\n      <th>state</th>\n      <th>dur</th>\n      <th>sbytes</th>\n      <th>dbytes</th>\n      <th>sttl</th>\n      <th>...</th>\n      <th>ct_ftp_cmd</th>\n      <th>ct_srv_src</th>\n      <th>ct_srv_dst</th>\n      <th>ct_dst_ltm</th>\n      <th>ct_src_ltm</th>\n      <th>ct_src_dport_ltm</th>\n      <th>ct_dst_sport_ltm</th>\n      <th>ct_dst_src_ltm</th>\n      <th>attack_cat</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59.166.0.0</td>\n      <td>1390</td>\n      <td>149.171.126.6</td>\n      <td>53</td>\n      <td>udp</td>\n      <td>CON</td>\n      <td>0.001055</td>\n      <td>132</td>\n      <td>164</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59.166.0.0</td>\n      <td>33661</td>\n      <td>149.171.126.9</td>\n      <td>1024</td>\n      <td>udp</td>\n      <td>CON</td>\n      <td>0.036133</td>\n      <td>528</td>\n      <td>304</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59.166.0.6</td>\n      <td>1464</td>\n      <td>149.171.126.7</td>\n      <td>53</td>\n      <td>udp</td>\n      <td>CON</td>\n      <td>0.001119</td>\n      <td>146</td>\n      <td>178</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>12</td>\n      <td>8</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>59.166.0.5</td>\n      <td>3593</td>\n      <td>149.171.126.5</td>\n      <td>53</td>\n      <td>udp</td>\n      <td>CON</td>\n      <td>0.001209</td>\n      <td>132</td>\n      <td>164</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>6</td>\n      <td>9</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59.166.0.3</td>\n      <td>49664</td>\n      <td>149.171.126.0</td>\n      <td>53</td>\n      <td>udp</td>\n      <td>CON</td>\n      <td>0.001169</td>\n      <td>146</td>\n      <td>178</td>\n      <td>31</td>\n      <td>...</td>\n      <td>0</td>\n      <td>7</td>\n      <td>9</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 49 columns</p>\n</div>"
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEUb71mo3kQ0"
      },
      "source": [
        "## Data cleaning and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emqivUeUKXsD"
      },
      "outputs": [],
      "source": [
        "# Splitting data into train and test\n",
        "# All the operation like cleaning, EDA and FE will be done on train data only\n",
        "\n",
        "train, test = train_test_split(all_data, test_size=0.3, random_state=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lot-WQk7yf0a"
      },
      "outputs": [],
      "source": [
        "# Deleting the concatenated dataframe, as we don't need that anymore\n",
        "del all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "BTFWAF16xm6G",
        "outputId": "44fcaac8-151c-4e2b-9183-664256f5ceb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1778032, 49) \n",
            " (762015, 49)\n"
          ]
        }
      ],
      "source": [
        "print(train.shape,'\\n',test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "h5DxlsTTHGgQ",
        "outputId": "78e91b5c-252f-47ff-b828-ae3e30ef3b03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Name\nsrcip                     0\nsport                     0\ndstip                     0\ndsport                    0\nproto                     0\nstate                     0\ndur                       0\nsbytes                    0\ndbytes                    0\nsttl                      0\ndttl                      0\nsloss                     0\ndloss                     0\nservice                   0\nsload                     0\ndload                     0\nspkts                     0\ndpkts                     0\nswin                      0\ndwin                      0\nstcpb                     0\ndtcpb                     0\nsmeansz                   0\ndmeansz                   0\ntrans_depth               0\nres_bdy_len               0\nsjit                      0\ndjit                      0\nstime                     0\nltime                     0\nsintpkt                   0\ndintpkt                   0\ntcprtt                    0\nsynack                    0\nackdat                    0\nis_sm_ips_ports           0\nct_state_ttl              0\nct_flw_http_mthd     943876\nis_ftp_login        1001037\nct_ftp_cmd                0\nct_srv_src                0\nct_srv_dst                0\nct_dst_ltm                0\nct_src_ltm                0\nct_src_dport_ltm          0\nct_dst_sport_ltm          0\nct_dst_src_ltm            0\nattack_cat          1552862\nlabel                     0\ndtype: int64"
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking for null values\n",
        "train.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lymw1lcbTBtk"
      },
      "source": [
        "### Filling Null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "owrNY7UeHxkW",
        "outputId": "84ecc39d-4d00-47b4-aa97-0975b1d71fef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "attack_cat\nGeneric             150906\nExploits             31507\n Fuzzers             13345\nDoS                  11433\n Reconnaissance       8544\n Fuzzers              3569\nAnalysis              1855\nBackdoor              1242\nReconnaissance        1220\n Shellcode             904\nBackdoors              374\nShellcode              151\nWorms                  120\nName: count, dtype: int64"
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['attack_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbjAKIGcICqY"
      },
      "outputs": [],
      "source": [
        "# We don't have \"normal\" values for \"attack_cat\", so we must fill Null values with \"normal\"\n",
        "train['attack_cat'] = train.attack_cat.fillna(value='normal').apply(lambda x: x.strip().lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "v2ZbFH8hILg9",
        "outputId": "5e840326-0e3c-43b3-bef4-3e49f9561679"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "attack_cat\nnormal            1552862\ngeneric            150906\nexploits            31507\nfuzzers             16914\ndos                 11433\nreconnaissance       9764\nanalysis             1855\nbackdoor             1242\nshellcode            1055\nbackdoors             374\nworms                 120\nName: count, dtype: int64"
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['attack_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAz36cZTHVYk"
      },
      "outputs": [],
      "source": [
        "train['ct_flw_http_mthd'] = train.ct_flw_http_mthd.fillna(value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "exliBT3ZIpUk",
        "outputId": "74ccb552-2a22-45e6-9c73-03f057822c05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "is_ftp_login\n0.0    746536\n1.0     30329\n4.0       109\n2.0        21\nName: count, dtype: int64"
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Even though it's a binary column, but there're values like 2 and 4\n",
        "train['is_ftp_login'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzY13jPlIuKR"
      },
      "outputs": [],
      "source": [
        "train['is_ftp_login'] = (train.is_ftp_login.fillna(value=0)).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "H_NVLazgKOth",
        "outputId": "ea044f5e-6080-47de-9c04-37410fa01cec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "0"
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# There are no Null values in the train data anymore\n",
        "train.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avzn0XAKq6r2"
      },
      "source": [
        "## Information about dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "WelMPaD4FLL1",
        "outputId": "efcc21e1-4409-4ea2-e139-6e6b9a790fee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes',\n       'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload',\n       'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz',\n       'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime',\n       'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',\n       'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n       'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm',\n       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat',\n       'label'],\n      dtype='object', name='Name')"
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting name of all the columns\n",
        "train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjtHGh5rF5M"
      },
      "source": [
        "### Class distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "-rA25dkVVlJp",
        "outputId": "4d6d714a-62d1-4ad1-f1d4-59d35b3bd08d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Train: there are 0.8733599845222133 % of class 0 and 0.12664001547778667 % of class 1\n",
            "In Test: there are 0.8738699369434985 % of class 0 and 0.1261300630565015 % of class 1\n"
          ]
        }
      ],
      "source": [
        "train_0, train_1 = train['label'].value_counts()[0] / len(train.index), train['label'].value_counts()[1] / len(train.index)\n",
        "test_0, test_1 = test['label'].value_counts()[0] / len(test.index), test['label'].value_counts()[1] / len(test.index)\n",
        "\n",
        "print(\"In Train: there are {} % of class 0 and {} % of class 1\".format(train_0, train_1))\n",
        "print(\"In Test: there are {} % of class 0 and {} % of class 1\".format(test_0, test_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "k-JZdqoLoNOQ",
        "outputId": "e913ad3a-c040-433f-e7fa-3605bb4d5140"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting target label distribution\n",
        "plt.figure()\n",
        "plt.title(\"class distribution of train and test dataset\")\n",
        "train['label'].value_counts().plot(kind=\"bar\", color='b', label=\"train\")\n",
        "test['label'].value_counts().plot(kind=\"bar\", color='orange', label=\"test\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYGJtnSXX4FQ"
      },
      "source": [
        "Both train and test datasets are highly imbalanced.<br>\n",
        "Percentage of points belongs to class 1 (attack) is very little (only 12.6 %) in both of them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfekDewOSL9K"
      },
      "source": [
        "### Datatype information of the Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DIQ4p178Mwt9",
        "outputId": "3dd224a7-6cf9-4737-e8a8-15b3bf0b36f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "(0, 1499013)"
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check if any null value present in datasets\n",
        "train.isnull().sum().sum(), test.isnull().sum().sum()  # No Null in train and test data is still raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "id": "94tDgeQmMri1",
        "outputId": "c8b78f9d-db43-46b8-966e-6c2ace92c95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1778032 entries, 81030 to 2079982\n",
            "Data columns (total 49 columns):\n",
            " #   Column            Dtype  \n",
            "---  ------            -----  \n",
            " 0   srcip             object \n",
            " 1   sport             object \n",
            " 2   dstip             object \n",
            " 3   dsport            object \n",
            " 4   proto             object \n",
            " 5   state             object \n",
            " 6   dur               float64\n",
            " 7   sbytes            int64  \n",
            " 8   dbytes            int64  \n",
            " 9   sttl              int64  \n",
            " 10  dttl              int64  \n",
            " 11  sloss             int64  \n",
            " 12  dloss             int64  \n",
            " 13  service           object \n",
            " 14  sload             float64\n",
            " 15  dload             float64\n",
            " 16  spkts             int64  \n",
            " 17  dpkts             int64  \n",
            " 18  swin              int64  \n",
            " 19  dwin              int64  \n",
            " 20  stcpb             int64  \n",
            " 21  dtcpb             int64  \n",
            " 22  smeansz           int64  \n",
            " 23  dmeansz           int64  \n",
            " 24  trans_depth       int64  \n",
            " 25  res_bdy_len       int64  \n",
            " 26  sjit              float64\n",
            " 27  djit              float64\n",
            " 28  stime             int64  \n",
            " 29  ltime             int64  \n",
            " 30  sintpkt           float64\n",
            " 31  dintpkt           float64\n",
            " 32  tcprtt            float64\n",
            " 33  synack            float64\n",
            " 34  ackdat            float64\n",
            " 35  is_sm_ips_ports   int64  \n",
            " 36  ct_state_ttl      int64  \n",
            " 37  ct_flw_http_mthd  float64\n",
            " 38  is_ftp_login      int32  \n",
            " 39  ct_ftp_cmd        object \n",
            " 40  ct_srv_src        int64  \n",
            " 41  ct_srv_dst        int64  \n",
            " 42  ct_dst_ltm        int64  \n",
            " 43  ct_src_ltm        int64  \n",
            " 44  ct_src_dport_ltm  int64  \n",
            " 45  ct_dst_sport_ltm  int64  \n",
            " 46  ct_dst_src_ltm    int64  \n",
            " 47  attack_cat        object \n",
            " 48  label             int64  \n",
            "dtypes: float64(11), int32(1), int64(28), object(9)\n",
            "memory usage: 671.5+ MB\n"
          ]
        }
      ],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "hpFAIry0RJ45",
        "outputId": "221bcde0-8200-40d2-8027-36f570b9338b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "int64      28\nfloat64    11\nobject      9\nint32       1\nName: count, dtype: int64"
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# All the datatypes in our dataset\n",
        "train.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "0EupesN7ORrD",
        "outputId": "5824ebd9-7920-4176-dde8-301f53e50459"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'service',\n       'ct_ftp_cmd', 'attack_cat'],\n      dtype='object', name='Name')"
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Categorical feature names\n",
        "train.select_dtypes(exclude=np.number).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "BUVY87vzLP5i",
        "outputId": "94c20644-47e3-493d-870d-63b02996a5d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "array([0, ' ', 1, '1', '0', 5, 2, 3, 4, '4', 6, '2', 8], dtype=object)"
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In the research paper it was mentioned that, this is a numerical feature not a categorical\n",
        "train['ct_ftp_cmd'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugmRdb8HLWb8"
      },
      "outputs": [],
      "source": [
        "# Removing empty space and converting it to numerical\n",
        "train['ct_ftp_cmd'] = train['ct_ftp_cmd'].replace(to_replace=' ', value=0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "jbCX_NQlLk2B",
        "outputId": "ce0e75d4-971b-432e-e1a0-2a6cca956f45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "array([0, 1, 5, 2, 3, 4, 6, 8])"
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['ct_ftp_cmd'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "3oIF3y7eLtV5",
        "outputId": "c778d9ca-1c51-457b-883d-7ab9409f82f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'service',\n       'attack_cat'],\n      dtype='object', name='Name')"
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Categorical feature names\n",
        "train.select_dtypes(exclude=np.number).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "eDg3n5hySsSz",
        "outputId": "fcaad993-927e-46e5-ce49-d5bda8f91229"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'sload',\n       'dload', 'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz',\n       'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime',\n       'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',\n       'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',\n       'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm',\n       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'label'],\n      dtype='object', name='Name')"
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Numeric features names\n",
        "train.select_dtypes(include=np.number).columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJT_4FIgRlTt"
      },
      "source": [
        "**Observations:**\n",
        "1. In this dataset there are mainly 2 types of data types present\n",
        "    - numerical\n",
        "    - categorical\n",
        "\n",
        "2. Categorical columns: 'proto', 'service', 'state'\n",
        "\n",
        "3. From the research paper we found that there are binary columns also-<br>\n",
        "nummerical but binary: 'is_sm_ips_ports', 'is_ftp_login'\n",
        "\n",
        "4. Numerical columns : 'id', 'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat',\n",
        "'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src',\n",
        "'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
        "'ct_dst_src_ltm', 'ct_ftp_cmd', 'ct_flw_http_mthd',\n",
        "'ct_src_ltm', 'ct_srv_dst'\n",
        "\n",
        "5. Target columns: 'attack_cat', 'label'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9qs5a1u-dVa"
      },
      "outputs": [],
      "source": [
        "# Info for test data transformation\n",
        "saved_dict['binary_col'] = ['is_sm_ips_ports', 'is_ftp_login']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38EtRlewUz2P"
      },
      "source": [
        "### Fixing values of the columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "RAhBgWXaPUfB",
        "outputId": "292c521e-44b9-43ff-8154-9a647378847f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_sm_ips_ports\n",
            "0    1775116\n",
            "1       2916\n",
            "Name: count, dtype: int64\n",
            "\n",
            "is_ftp_login\n",
            "0    1747573\n",
            "1      30329\n",
            "4        109\n",
            "2         21\n",
            "Name: count, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# see if binary columns are really binary\n",
        "for col in 'is_sm_ips_ports', 'is_ftp_login':\n",
        "    print(train[col].value_counts())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgMjHjzsT_Kk"
      },
      "source": [
        "The col \"is_ftp_login\" has few wrong values like 2, 4. It should only have 0 and 1, If the ftp session is accessed by user and\n",
        "password then 1 else 0. Need to fix this.\n",
        "\n",
        "Replacing all the values apart from 0 and 1 from these two columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nwgE2AkL1nj"
      },
      "outputs": [],
      "source": [
        "train['is_ftp_login'] = np.where(train['is_ftp_login']>1, 1, train['is_ftp_login'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "tbK1cgrRML5S",
        "outputId": "7b75650f-95fc-457e-f8b1-0fc869d8c98b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "is_ftp_login\n0    1747573\n1      30459\nName: count, dtype: int64"
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['is_ftp_login'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfdckvbXsYJL"
      },
      "source": [
        "In the feature \"service\" we have \"-\" we need to replace that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "SkHsM3AMsgLR",
        "outputId": "dfe7968e-e9e0-4fd1-8176-4ce979e2992c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "service\n-           872286\ndns         547094\nhttp        144555\nftp-data     88310\nsmtp         57160\nftp          34293\nssh          32897\npop3          1076\ndhcp           124\nssl            109\nsnmp            81\nradius          27\nirc             20\nName: count, dtype: int64"
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['service'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6-UD3X1snWi"
      },
      "outputs": [],
      "source": [
        "# removing all the \"-\" and replacing those with \"None\"\n",
        "train['service'] = train['service'].apply(lambda x:\"None\" if x==\"-\" else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "z3ArmRX6NeW4",
        "outputId": "bf547f13-96df-4b14-90ae-fe445e58f5a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "service\nNone        872286\ndns         547094\nhttp        144555\nftp-data     88310\nsmtp         57160\nftp          34293\nssh          32897\npop3          1076\ndhcp           124\nssl            109\nsnmp            81\nradius          27\nirc             20\nName: count, dtype: int64"
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['service'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "duQJ_2nV5CBE",
        "outputId": "0c1da48e-25b8-4832-fef9-2930f4226faf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "11"
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In the research paper there are not 10 unique values\n",
        "train['attack_cat'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "1RJ7yvY91dju",
        "outputId": "2637ce28-d60a-4ebe-84d1-87c492172061"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "attack_cat\nnormal            1552862\ngeneric            150906\nexploits            31507\nfuzzers             16914\ndos                 11433\nreconnaissance       9764\nanalysis             1855\nbackdoor             1242\nshellcode            1055\nbackdoors             374\nworms                 120\nName: count, dtype: int64"
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['attack_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NFsave95CBc"
      },
      "outputs": [],
      "source": [
        "train['attack_cat'] = train['attack_cat'].replace('backdoors','backdoor', regex=True).apply(lambda x: x.strip().lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOKmuHehlvmV"
      },
      "source": [
        "## Saving files to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "l-JeWiso4J14",
        "outputId": "bc5c9a14-cb03-418c-9e86-b16cac845f90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "((1778032, 49), (762015, 49))"
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape, test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4nDrjgasnML"
      },
      "outputs": [],
      "source": [
        "train.to_csv('C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/target/train_alldata_EDA.csv', index=False)\n",
        "test.to_csv('C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/target/test_alldata_EDA.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMkN5-dSQdOk"
      },
      "outputs": [],
      "source": [
        "pickle.dump(saved_dict, open('C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/target/final_ipynb', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2RqN_AxiZBw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNSW-NB15: Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "3cC6wS2_uOZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing EDA on the dataset in this notebook.\n",
        "\n",
        "Below is my plan:\n",
        "1. Correlation of the features and its heatmap\n",
        "2. Pairplot between highly correlated features.\n",
        "3. For all the categorial or columns with small number of unique values, countplot or barplot of those features.\n",
        "4. For any numerical features, plot pdf"
      ],
      "metadata": {
        "id": "oovDYrGQuPho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data from disk"
      ],
      "metadata": {
        "id": "HkGKneZPucDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/target/train_alldata_EDA.csv')\n",
        "test = pd.read_csv('C:/Users/DELL/PycharmProjects/Intrusion Detection System- UNSW_NB15/target/test_alldata_EDA.csv')"
      ],
      "metadata": {
        "id": "3QXyjz67uSjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "GomIhueVuWcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing the train data into attack and non-attack category\n",
        "normal = train[train['label']==0]\n",
        "anomaly = train[train['label']==1]"
      ],
      "metadata": {
        "id": "cpevwH7QuYoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation\n",
        "Getting correlation values for all the features.\n",
        "\n",
        "Plot heatmap of correaltion for better visualization"
      ],
      "metadata": {
        "id": "CXWvcDjauemT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions"
      ],
      "metadata": {
        "id": "K1EwDSfBuicm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_corr(col1, col2=\"label\", df=train):\n",
        "    '''\n",
        "    This function returns correlation between 2 given features.\n",
        "    Also gives corr of the given features with \"label\" afetr applying log1p to it.\n",
        "    '''\n",
        "    corr = df[[col1, col2]].corr().iloc[0,1]\n",
        "    log_corr = df[col1].apply(np.log1p).corr(df[col2])\n",
        "\n",
        "    print(\"Correlation : {}\\nlog_Correlation: {}\".format(corr, log_corr))"
      ],
      "metadata": {
        "id": "vk5MwZFWukiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corr(col1, col2=\"label\", df=train):\n",
        "    \"\"\"\n",
        "    This function returns correlation between 2 given features\n",
        "    \"\"\"\n",
        "    return df[[col1, col2]].corr().iloc[0,1]"
      ],
      "metadata": {
        "id": "eXrrUHP7ul8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the correlation matrix of the dataset\n",
        "# Refer: https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf\n",
        "#\n",
        "method = \"pearson\"\n",
        "# # correlation matrix\n",
        "# corr_mat = train.corr(method=method)\n",
        "#\n",
        "# plt.figure(figsize=(12,12))\n",
        "# sns.heatmap(corr_mat, square=True)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Convert non-numeric values to NaN\n",
        "train = train.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Now, calculate the correlation matrix\n",
        "corr_mat = train.corr(method=method)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12,12))\n",
        "sns.heatmap(corr_mat, square=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wv588uVouovh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting and printing high correlated features\n",
        "limit = 0.9\n",
        "\n",
        "columns = corr_mat.columns\n",
        "for i in range(corr_mat.shape[0]):\n",
        "    for j in range(i+1, corr_mat.shape[0]):\n",
        "        if corr_mat.iloc[i, j] >= 0.9:\n",
        "            print(f\"{columns[i]:20s} {columns[j]:20s} {corr_mat.iloc[i, j]}\")\n"
      ],
      "metadata": {
        "id": "GVeRFs9Zur1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most correlated features are:\n",
        "* sbytes, sloss\n",
        "* dpkts, dbytes, dloss\n",
        "* sttl, ct_state_ttl, label\n",
        "* swin, dwin\n",
        "* stime, dtime\n",
        "* tcprtt, synack, ackdat\n",
        "* ct_srv_src, ct_dst_src_ltm, ct_srv_dst\n",
        "* ct_dst_ltm, ct_src_ltm, ct_src_dport_ltm, ct_dst_sport_ltm\n",
        "\n"
      ],
      "metadata": {
        "id": "Qj8CHS-yuuDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pairplots of highly correlated features"
      ],
      "metadata": {
        "id": "b_f6WvCRuw33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sbytes and sloss\n",
        "1. These features are having very high corr between them more 95%\n",
        "2. All the features are following a straight line except for the starting points that means they are having incresing relation with each other."
      ],
      "metadata": {
        "id": "hatt33v_uxed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"whitegrid\")\n",
        "sns.pairplot(data=train, vars=['sbytes', 'sloss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jEgUEmyAuzYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dpkts, dbytes and dloss\n",
        "1. These features are also showing same charecteristics as above.\n",
        "2. \"dpkts\" and \"dbytes\" are following a strainght line except for first few point and have 97% corr.\n",
        "3. \"dloss\" and \"dbytes\" are completely following straight line with more than 99% corr."
      ],
      "metadata": {
        "id": "tHxXUJ_su04t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.pairplot(train, vars=['dpkts', 'dbytes', 'dloss'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N1yvnFKXu27E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sttl, ct_state_ttl, label\n",
        "1. sttl and ct_state_ttl are having 90% correlation between them.\n",
        "\n",
        "But one important thing to notice that these 2 features are also highly correlated with target features i.e, label.\n",
        "\n",
        "Going to drop 1 feature, and the remaining feature could play a vital role for the classification task."
      ],
      "metadata": {
        "id": "H-bLln1bu44e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.pairplot(train, vars=['sttl', 'ct_state_ttl', 'label'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CzxDDduPu60-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### swin, dwin\n",
        "swin: Source TCP window advertisement, dwin: Destination TCP window advertisement\n",
        "\n",
        "Correlation values is 99% between them. Even though these 2 columns are numerical but most of their values are only 0 and 255."
      ],
      "metadata": {
        "id": "jvuV96OJu9CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.pairplot(train, vars=['swin', 'dwin'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aGarxqelu_Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tcprtt, synack and ackdat\n",
        "synack: The time between the SYN and the SYN_ACK packets of the TCP.\n",
        "<br>\n",
        "ackdat: The time between the SYN_ACK and the ACK ackets of the TCP.\n",
        "<br>\n",
        "tcprtt: sum of above 2 columns.\n",
        "\n",
        "1. There are some similarity between \"tcprtt\" and \"synack\" and \"tcsprtt\" and \"actdat\" as value of x increases value of y also increases.\n",
        "2. But for \"synack\" and \"ackdat\" values are scattered all over. And its corr value also not over 90%"
      ],
      "metadata": {
        "id": "_EyTJQX_vAaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.pairplot(train, vars=['tcprtt', 'synack', 'ackdat'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "27HRp2BqvC2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ct_srv_src, ct_srv_dst and ct_dst_src_ltm\n",
        "1. Range of all the features are between 0 to 60.\n",
        "2. Most of the values are close to 0 and less than 10.\n",
        "3. Values are well scattered but there are clear line that means have some linear relationship."
      ],
      "metadata": {
        "id": "zKI84t7RvEq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(train, vars=['ct_srv_src', 'ct_srv_dst', 'ct_dst_src_ltm'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UN9TUEaUvGN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ct_dst_ltm, ct_src_ltm, ct_src_dport_ltm and ct_dst_sport_ltm\n",
        "1. values range from 0 to 60.\n",
        "2. Most of the values are close to 0 and lesser than 20.\n",
        "3. ct_dst_ltm highly corr with ct_dst_sport_ltm\n",
        "4. ct_src_ltm corr with ct_src_dport_ltm\n",
        "5. ct_src_dport_ltm corr with ct_dst_sport_ltm"
      ],
      "metadata": {
        "id": "qP1MGHUIvIxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(train, vars=['ct_dst_ltm', 'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Shn0ny1ZvKtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Countplots and pdf of features"
      ],
      "metadata": {
        "id": "c0aI6wMbvUGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility functions"
      ],
      "metadata": {
        "id": "lxz2fMTOvUko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def col_countplot(col, train_data=train):\n",
        "    \"\"\"\n",
        "    This function plots countplot of a given feature for train dataset\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8,4))\n",
        "    sns.set_style('whitegrid')\n",
        "    # countplot of the given column\n",
        "    ax = sns.countplot(x=col, hue='label', data=train_data)\n",
        "    ax.legend(loc=\"upper right\", labels=('normal', 'attack'))\n",
        "    ax.set_title(\"train data\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pP2MdXjivWu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting pdf of numerical columns\n",
        "# Refer: https://www.kaggle.com/khairulislam/unsw-nb15-eda\n",
        "\n",
        "def dual_plot(col, data1=normal, data2=anomaly, label1='normal', label2='anomaly', method=None):\n",
        "    \"\"\"\n",
        "    This function plots pdf of the given feature on attack and non-attck data\n",
        "    \"\"\"\n",
        "    if method != None:\n",
        "        sns.set_style('whitegrid')\n",
        "        sns.distplot(data1[col].apply(method), label=label1, hist=False, rug=True)\n",
        "        sns.distplot(data2[col].apply(method), label=label2, hist=False, rug=True)\n",
        "    else:\n",
        "        sns.set_style('whitegrid')\n",
        "        sns.distplot(data1[col], label=label1, hist=False, rug=True)\n",
        "        sns.distplot(data2[col], label=label2, hist=False, rug=True)\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "QaHq5HF3vYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_column(s, to_keep):\n",
        "    \"\"\"\n",
        "    This function reduce categorical values of a given feature to values provided in to_keep list\n",
        "    and make every other value \"others\"\n",
        "    \"\"\"\n",
        "    s = s.lower().strip()\n",
        "    if s not in to_keep:\n",
        "        return \"others\"\n",
        "    else:\n",
        "        return s"
      ],
      "metadata": {
        "id": "2DJj7WYOvaRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attack Category\n",
        "The name of each attack category. In this\n",
        "data set, there are total nine categories of attack and normal is non-attack.\n",
        "\n",
        "The data is highly imbalanced and have lots of non-attack than attacks.\n",
        "\n",
        "\n",
        "In train data most occured attack data categories are \"Generic\", \"Exploits\", Fuzzers\", \"DoS\" and \"Reconnaissance\""
      ],
      "metadata": {
        "id": "_Ztd_h-Fvb5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('attack_cat')"
      ],
      "metadata": {
        "id": "vkLQIwb2vd4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Proto\n",
        "Categorical feature. Transaction protocol\n",
        "\n",
        "\n",
        "1. The no of unique values of 'proto' in dataset where attack is normal is 7 but in anamoly category its 129. So to plot it we are reducing the no of values.\n",
        "2. Most of the values are consists of udp and tcp\n",
        "3. For non-attacks count of tcp is lot higher\n",
        "4. Considering the imbalce there are lots of udp cat for attacks"
      ],
      "metadata": {
        "id": "BIabc7Pqvfn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['proto'].nunique()"
      ],
      "metadata": {
        "id": "NWkBETSHviv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal['proto'].nunique(), anomaly['proto'].nunique()"
      ],
      "metadata": {
        "id": "0O1sZTlovkL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['proto'].value_counts().head(10)*100/train.shape[0]"
      ],
      "metadata": {
        "id": "axy48iJQvlUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_keep = ['tcp', 'udp', 'unas', 'arp', 'ospf']\n",
        "train['proto_reduced'] = train['proto'].apply(reduce_column, args=(to_keep,))"
      ],
      "metadata": {
        "id": "dx07imbtvmhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('proto_reduced')"
      ],
      "metadata": {
        "id": "SayZcyravoXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Service\n",
        "Categorical Features\n",
        "\n",
        "\n",
        "1. In our dataset we can value \"-\" that is null, we need to remove that and replace it.\n",
        "2. For normal there are lots of others that means most of \"-\" and few rare values.\n",
        "3. In attack data \"dns\" is present higher than any other values. There are few no of others and http also."
      ],
      "metadata": {
        "id": "Cwg0uA1vvpue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['service'].nunique()"
      ],
      "metadata": {
        "id": "M5zzwM8vvsLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['service'].value_counts()"
      ],
      "metadata": {
        "id": "GASdVO7bvtVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_keep = ['dns', 'http', 'smtp', 'ftp-data', 'ftp', 'ssh', 'pop3']\n",
        "train['service_reduced'] = train['service'].apply(reduce_column, args=(to_keep,))"
      ],
      "metadata": {
        "id": "WdNiZRQ7vuY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('service_reduced')"
      ],
      "metadata": {
        "id": "DkWq7tQhvveb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### State\n",
        "Categorical feature. The state and its dependent protocol\n",
        "\n",
        "Reducing very rare values with \"others\" to plot and visualize better\n",
        "\n",
        "1. For non-attacks \"fin\" is very frequent then \"cons\" almost half of \"fin\" and few \"int\" also.\n",
        "2. In attack \"int\" is higher than normal cat values. That could be really important feature to detect. There are very few \"fin\""
      ],
      "metadata": {
        "id": "hD2Hf35jvyHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['state'].nunique()"
      ],
      "metadata": {
        "id": "fwdtTUFRvzo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['state'].value_counts()"
      ],
      "metadata": {
        "id": "dsVJ1Pw6v1Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_keep = ['int', 'fin', 'con', 'req']\n",
        "train['state_reduced'] = train['state'].apply(reduce_column, args=(to_keep,))"
      ],
      "metadata": {
        "id": "fVkHXnNcv2oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('state_reduced')"
      ],
      "metadata": {
        "id": "gc28fVuwv4Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop(columns=['proto_reduced', 'state_reduced', 'service_reduced'], inplace=True)"
      ],
      "metadata": {
        "id": "wPohWQ_Kv5qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "D1C4pHQfv67f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ct_dst_ltm\n",
        "Numerical feature.<br>\n",
        "No. of connections of the same\n",
        "destination address in 100 connections\n",
        "according to the last time.\n",
        "\n",
        "1. Range of values of this feature is about 0 to 70\n",
        "2. For normal data most of the values are between 0 to 10 and few from 10 to 20.\n",
        "3. For anomaly values are close to 0 and also there are values from 10 to 30.\n",
        "4. Can visualize better after using log1p on the feature"
      ],
      "metadata": {
        "id": "ZqfExQClv81i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'ct_dst_ltm'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "q7q1SuQdv_Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8knN71zGwBDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### ct_flw_http_mthd\n",
        "Numerical feature with small discreate set of values.<br>\n",
        "No. of flows that has methods such as Get\n",
        "and Post in http service.\n",
        "\n",
        "1. For normal almost all the values are 0 and few 1.\n",
        "2. In attack data everything is 0 and there is very very litle no of 1's"
      ],
      "metadata": {
        "id": "LsL3NH6PwDd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['ct_flw_http_mthd'].nunique()"
      ],
      "metadata": {
        "id": "aevr4U1XwHn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('ct_flw_http_mthd')"
      ],
      "metadata": {
        "id": "OGe1YN_zwJA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ct_srv_src\n",
        "Numerical feature.<br>\n",
        "No. of connections that contain the same\n",
        "service and source address in 100\n",
        "connections according to the last time.\n",
        "1. Range of the values is between 0 to 70.\n",
        "2. For normal most of the values are from 0 to 15.\n",
        "3. For anomaly most of the values are from 0 to 10 and the values are also distributed between 15 to 50."
      ],
      "metadata": {
        "id": "PBVJOEHowNC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'ct_srv_src'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "xF7--IHIwPlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8jtFWGe2wRY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ct_state_ttl\n",
        "Numerical feature with small discrete set of values.\n",
        "1. normal data has 0 as most of its values.\n",
        "2. anomaly has most of its value 2.\n",
        "3. There are few attack with value 1 to and also very little non-attack with 1."
      ],
      "metadata": {
        "id": "ygztrqeNwSlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['ct_state_ttl'].nunique()"
      ],
      "metadata": {
        "id": "U2SYkUk0wVOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('ct_state_ttl')"
      ],
      "metadata": {
        "id": "d7ua2VsFwWo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dinpkt\n",
        "Numerical feature. Destination inter-packet arrival time (mSec)\n",
        "1. This feature has 55 unique values.\n",
        "2. Most of the non-attack data has value 1 and there are very very litte 2,3,4, 19,21,23 but they are very less in number not visible in the graoh also.\n",
        "3. Attck data has value 0 most no of time but that is also way too little compared with non-attack data."
      ],
      "metadata": {
        "id": "gm4ZaAynwZEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ct_dst_sport_ltm"
      ],
      "metadata": {
        "id": "168RCqZiwcdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['ct_dst_sport_ltm'].nunique()"
      ],
      "metadata": {
        "id": "KXQI9GnkwaA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('ct_dst_sport_ltm')"
      ],
      "metadata": {
        "id": "GqOwMUVTwemD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dintpkt\n",
        "Numerical feature.\n",
        "<br>\n",
        "Destination inter-packet arrival time (mSec)\n",
        "1. Feature has large range of values upto 60,000\n",
        "2. For nonmal data there is a very high peek from 15,000 to 20,000\n",
        "3. For attack data values are distributed in first 10000"
      ],
      "metadata": {
        "id": "b4SSYKWcwg65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['dintpkt'].nunique()"
      ],
      "metadata": {
        "id": "ClmU7fHKwoaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot('dintpkt')\n",
        "plt.subplot(122)\n",
        "dual_plot('dintpkt', method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_IDTyCaowpMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### djit\n",
        "Numerical feature. Destination jitter (mSec)\n",
        "1. Range of this feature is huge upto 800,000.\n",
        "2. Normal data distributed over very wide range upto 200,000.\n",
        "3. For attack data there is a huge peek close to 0 and distributioon of values are very narrow."
      ],
      "metadata": {
        "id": "0rCuGBwpwqvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['djit'].nunique()"
      ],
      "metadata": {
        "id": "EvqJHhy7wtHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot('djit')\n",
        "plt.subplot(122)\n",
        "dual_plot('djit', method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C6g0j8TGxUm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dload\n",
        "Numerical feature. Destination bits per second\n",
        "1. This feature has high correlation with target feat, 0.35\n",
        "2. Feature has huge range of values upto 1e8\n",
        "3. We can visualize better in log scale. For normal data they are distributed all over, has values close to 0 and also very large values\n",
        "4. And for attack data all the values are very close to 0. IN log scale we can see that values are between 3 to 15"
      ],
      "metadata": {
        "id": "_HlKhs3BxlfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['dload'].nunique()"
      ],
      "metadata": {
        "id": "ePKt0b0UxlH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot('dload')\n",
        "plt.subplot(122)\n",
        "dual_plot('dload', method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TlzrYAZWxrAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dbytes\n",
        "Numerical feature. Destination to source bytes.\n",
        "1. The range of values for this feature is very wide upto 1e7.\n",
        "2. Values of normal categories are grouped together. In log scale can see there are values close to 0 and then from 5 to 12.\n",
        "3. For attck category data values are widely spread. In log scale can see high peeks around 5 to 7."
      ],
      "metadata": {
        "id": "5YeoWMMKxuKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'dbytes'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "3p9ndo1YxtvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EVnYEmSjxxSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dmeansz\n",
        "Numerical feature. Mean of the flow packet size transmitted by the dst\n",
        "1. Feature range between 0 to 1600, with high corr with \"label\" 0.29\n",
        "2. For non-attck data most values are between 0-200. And few of them are distributed from 400 to 800\n",
        "3. Most of the values in anomaly from 0 to 100."
      ],
      "metadata": {
        "id": "nMk5RP_ixzvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'dmeansz'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "B8rhX2Jcx0Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HLgVXZJsx2aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dpkts\n",
        "Numerical feature. Destination to source packet count\n",
        "1. Range of values for this feature is around 10,000\n",
        "2. Non-attck cat values are very close to 0. In log scale its upto 5.\n",
        "2. Attack cat values are widely distributed and has higher fraction around 2 and 3 in log1p graph"
      ],
      "metadata": {
        "id": "1pIxsgsux6O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['dpkts'].nunique()"
      ],
      "metadata": {
        "id": "9vzluk1zx9qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot('dpkts')\n",
        "plt.subplot(122)\n",
        "dual_plot('dpkts', method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QHCGV_Wmx_Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dtcpb\n",
        "Numerical features. Destination TCP sequence number\n",
        "1. The range of this feature is very very wide 4*10^9\n",
        "2. Values of normal cat are lot more wide spread than anomaly cat.\n",
        "3. From the graphs its not much clear about the distribution.  But can see attack cat values are distributed through out total range."
      ],
      "metadata": {
        "id": "EmHAK5IdyBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'dtcpb'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "WoGiD3mQyCE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dlgDmdf_yEGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dttl\n",
        "Destination to source time to live\n",
        "<br>\n",
        "Numerical feature with only 11 unique value. Can plot it as categorical feature.\n",
        "\n",
        "\n",
        "Destination to source time to live\n",
        "1. MOst of the values for non-attack data is 29. There are some 0 and very few 252.\n",
        "2. There lots of 0 in attack data, no of 0 in attack is more than non-attack, there are few 252 also and that is also higher than non-attack."
      ],
      "metadata": {
        "id": "T1Xk3xY_yH61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['dttl'].nunique()"
      ],
      "metadata": {
        "id": "9rZZeBsRyHkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('dttl')"
      ],
      "metadata": {
        "id": "bO-CUbtKyKu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dur\n",
        "Numerical feature. Record total duration\n",
        "1. Range of this feature is 0 to 8000\n",
        "2. Both attack and non-attack cat values are very close to 0.\n",
        "3. But attack cat values are widely distributed than non-attack."
      ],
      "metadata": {
        "id": "efTks_loyNXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'dur'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "cOUDsdZnyTSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cOyHxQZOyTfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sintpkt\n",
        "Numerical feature. Source inter-packet arrival time (mSec)\n",
        "1. Most of the values in normat cat is close to 0 and spread to 400. And few values are also close to max value of the feature.\n",
        "2. anomaly cat has most of values near to 0 and spread is less compared to normal"
      ],
      "metadata": {
        "id": "_44-v41SyZXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['sintpkt'].nunique()"
      ],
      "metadata": {
        "id": "ToTP0uDLycB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot('sintpkt')\n",
        "plt.subplot(122)\n",
        "dual_plot('sintpkt', method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GbSHOhx8yd7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sttl\n",
        "Numerical feature with only 13 unique value.\n",
        "<br>\n",
        "Source to destination time to live\n",
        "1. normal cat data has most of its values 31 and few 60.\n",
        "2. anomaly cat has almost all of its value 254."
      ],
      "metadata": {
        "id": "mX1b_30IykoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['sttl'].nunique()"
      ],
      "metadata": {
        "id": "Ix687lEgylEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot(\"sttl\")"
      ],
      "metadata": {
        "id": "t2I6eZojym_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### stcpb\n",
        "Numerical feature. Source TCP sequence number\n",
        "1. The graphs are very similar to 'dtcpb'\n",
        "1. normal cat has higher no of values close to 0 and has spread through out whole range\n",
        "2. anomaly has most of its values near 0 and less spread compared to normal cat.\n"
      ],
      "metadata": {
        "id": "yRvVH8AMypNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'stcpb'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "NvemQ0R2ytSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hG9cyfQsyumq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### swin\n",
        "Numerical feature with 22 unique value.\n",
        "<br>\n",
        "Source TCP window advertisement\n",
        "1. Most of the values of this feature are just 0 and 255\n",
        "2. Normal cat data has most of its values 255 and 0\n",
        "3. Attack act has large no of 0 and small no of 255, just the inverse of normal cat."
      ],
      "metadata": {
        "id": "Bxmfyh7wyyNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['swin'].nunique()"
      ],
      "metadata": {
        "id": "ywUV_susy1sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_countplot('swin')"
      ],
      "metadata": {
        "id": "ZzxsgTHty23A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sbytes\n",
        "Source to destination bytes\n",
        "<br>\n",
        "Numerical feature with large range of values upto 1e7\n",
        "1. Most of normal cat values are close to 0.\n",
        "2. Attack cat has most of its values around 5 in log1p graph.\n",
        "3. The spread of values is wider in attack compared to normal."
      ],
      "metadata": {
        "id": "0SH5Cyz5y6Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'sbytes'\n",
        "train[col].nunique()"
      ],
      "metadata": {
        "id": "1xQsziFsy-ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "plt.subplot(121)\n",
        "dual_plot(col)\n",
        "plt.subplot(122)\n",
        "dual_plot(col, method=np.log1p)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uj51QP5gy_pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNSW-NB15: Feature Engineering"
      ],
      "metadata": {
        "id": "JgKpN49SzQjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data from disk\n",
        "train = pd.read_csv('./train_alldata_EDA.csv')\n",
        "test = pd.read_csv('./test_alldata_EDA.csv')"
      ],
      "metadata": {
        "id": "SN5zlqckzTot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function\n",
        "def multi_corr(col1, col2=\"label\", df=train):\n",
        "    '''\n",
        "    This function returns correlation between 2 given features.\n",
        "    Also gives corr of the given features with \"label\" afetr applying log1p to it.\n",
        "    '''\n",
        "    corr = df[[col1, col2]].corr().iloc[0,1]\n",
        "    log_corr = df[col1].apply(np.log1p).corr(df[col2])\n",
        "\n",
        "    print(\"Correlation : {}\\nlog_Correlation: {}\".format(corr, log_corr))"
      ],
      "metadata": {
        "id": "SUiLYsArzdUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def corr(col1, col2=\"label\", df=train):\n",
        "    \"\"\"\n",
        "    This function returns correlation between 2 given features\n",
        "    \"\"\"\n",
        "    return df[[col1, col2]].corr().iloc[0,1]"
      ],
      "metadata": {
        "id": "owccQVlPzfou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing highly correlated features"
      ],
      "metadata": {
        "id": "xpp1kB22zhR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting all the features with high correlation values with other features\n",
        "# Refer: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
        "corr_matrix = train.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.9\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
      ],
      "metadata": {
        "id": "DWvSbN5dzm91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't want to use these features for plotting because these are having high corr\n",
        "# And most likely have same kind of plots with already plotted feature\n",
        "print(to_drop)"
      ],
      "metadata": {
        "id": "vAMPiSSvzo3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_dict['corr_col'] = to_drop"
      ],
      "metadata": {
        "id": "9Rn27CDzzrQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the features from train and test data\n",
        "train.drop(columns=to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "JleSnmAnzsHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "dhmPETTZzuFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding New Features\n",
        "\n",
        "Refer: https://www.elastic.co/guide/en/ecs/master/ecs-network.html\n",
        "* Network bytes: Total bytes trasferred by the network. It is sum of 'sbytes' (Source to destination bytes) and 'dbytes' (Destination to source bytes)."
      ],
      "metadata": {
        "id": "b123KLCLzwhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating new features\n",
        "train['network_bytes'] = train['sbytes'] + train['dbytes']"
      ],
      "metadata": {
        "id": "dw803Gkfzxfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "JgDSiz7Cz0lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping columns which are not useful for the classification\n",
        "# attack_cat is for multiclass classification\n",
        "# all the other columns are address related and not present in sample train data\n",
        "train.drop(['srcip', 'sport', 'dstip', 'dsport', 'attack_cat'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "TLMnMVz_z2b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use during test data transformation\n",
        "saved_dict['to_drop'] = ['srcip', 'sport', 'dstip', 'dsport', 'attack_cat']"
      ],
      "metadata": {
        "id": "DGDjcm3Pz3q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "hrBGJYqmz4_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying log1p on Numerical columns\n",
        "\n",
        "During EDA we found that few numerical columns shows better visualization for pdf curves if we apply log1p to the columns.\n",
        "\n",
        "So I thought to try log1p on all the columns and check the correlation value of the original column and log1p column with target column i.e. \"label\""
      ],
      "metadata": {
        "id": "QAo3t5J6z7s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting number of unique values of all the columns\n",
        "# If the unique values are high that means it has continuous set of values\n",
        "col_unique_values = train.nunique()"
      ],
      "metadata": {
        "id": "hYoYPUzfz9mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the unique values are getter than some threshould than we will check its corr\n",
        "col = col_unique_values[col_unique_values>200].index"
      ],
      "metadata": {
        "id": "fpyYz8F1z-8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking corr value of original col and log1p applied col\n",
        "# Taking those columns whose unique values are getter than some threshould\n",
        "for column in col:\n",
        "    print(\"{:-^30}\".format(column))\n",
        "    multi_corr(column)"
      ],
      "metadata": {
        "id": "d7A46Ir50Are"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will apply log1p on this columns and remove original columns\n",
        "log1p_col = ['dur', 'sbytes', 'dbytes', 'sload', 'dload', 'spkts', 'stcpb', 'dtcpb', 'smeansz', 'dmeansz', 'sjit', 'djit', 'network_bytes']"
      ],
      "metadata": {
        "id": "8yJiUVDj0C3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_dict['log1p_col'] = log1p_col"
      ],
      "metadata": {
        "id": "OaKhn4ya0ECC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mode values of every features, will use to fill Null values of test\n",
        "mode_dict = train.mode().iloc[0].to_dict()"
      ],
      "metadata": {
        "id": "4C3Jfvfg0FHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log1p_transform(col, df=train):\n",
        "    '''\n",
        "    Apply log1p on given column.\n",
        "    Remove the original cola and keep log1p applied col\n",
        "    '''\n",
        "    new_col = col+'_log1p'\n",
        "    df[new_col] = df[col].apply(np.log1p)\n",
        "    df.drop(col, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "mduh7x1J0GjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming columns with log1p\n",
        "for col in log1p_col:\n",
        "    log1p_transform(col, df=train)"
      ],
      "metadata": {
        "id": "o3n1jb6V0TR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "WYMvdqfx0Ud9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "id": "1VwwUC9x0VyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "VcBAZkUR0XA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating x and y set from the dataset\n",
        "x_train, y_train = train.drop(columns=['label']), train['label']\n",
        "x_test, y_test = test.drop(columns=['label']), test['label']"
      ],
      "metadata": {
        "id": "fx853aWy0YPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print()\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "Mbe7qf9Y0Zxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving all the files to disk to use later\n",
        "pickle.dump((x_train, y_train), open('.final_ipynb/final_train.pkl', 'wb'))\n",
        "pickle.dump((x_test, y_test), open('.final_ipynb/final_test.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "7tayMZzb0bgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting categorical and numerical columns in 2 diff lists\n",
        "cat_col = ['proto', 'service', 'state']\n",
        "num_col = list(set(x_train.columns) - set(cat_col))"
      ],
      "metadata": {
        "id": "L61Jiznb0ckP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use later, during test data cleaning\n",
        "saved_dict['cat_col'] = cat_col\n",
        "saved_dict['num_col'] = num_col"
      ],
      "metadata": {
        "id": "5fkDYwOW0d5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "9HJhDIVm0fv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing\n",
        "    \n",
        "As we have seen that the range of few features in this dataset is very large. So we will keep everything within certain range by applying standardscaler. After this all the features will have mean 0 and std 1"
      ],
      "metadata": {
        "id": "n3cUNboN0jXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(x_train[num_col])"
      ],
      "metadata": {
        "id": "NIPwiVGA0lD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[num_col] = scaler.transform(x_train[num_col])"
      ],
      "metadata": {
        "id": "EFCShiK_0mi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "J48veeGg0obh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Onehot Encoding\n",
        "\n",
        "In our dataset we have few categorical columns with text data.\n",
        "But ML models can't process text data it can process numbers.\n",
        "\n",
        "So we have to convert categorical columns to numerical columns in some way.\n",
        "We will use onehotencoder where we will assign 1 if the value is present for the row and rest of the columns will be 0."
      ],
      "metadata": {
        "id": "RigF1AMU0qMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Onehot Encoding\n",
        "service_ = OneHotEncoder()\n",
        "proto_ = OneHotEncoder()\n",
        "state_ = OneHotEncoder()\n",
        "ohe_service = service_.fit(x_train.service.values.reshape(-1,1))\n",
        "ohe_proto = proto_.fit(x_train.proto.values.reshape(-1,1))\n",
        "ohe_state = state_.fit(x_train.state.values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "Cojk-MK60sLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are onehot encoding the given column\n",
        "# Remove the original categorical column\n",
        "for col, ohe in zip(['proto', 'service', 'state'], [ohe_proto, ohe_service, ohe_state]):\n",
        "    x = ohe.transform(x_train[col].values.reshape(-1,1))\n",
        "    tmp_df = pd.DataFrame(x.todense(), columns=[col+'_'+i for i in ohe.categories_[0]])\n",
        "    x_train = pd.concat([x_train.drop(col, axis=1), tmp_df], axis=1)"
      ],
      "metadata": {
        "id": "FXY4STcW0s_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "biK_lvF80ul4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving** all the important parameters and objects to disk so that we can apply same process on test data"
      ],
      "metadata": {
        "id": "qTkEWkcT0z3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'final_ipynb/'"
      ],
      "metadata": {
        "id": "GmZXwUxG01E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(scaler, open(file_path+'scaler.pkl', 'wb'))  # Standard scaler\n",
        "pickle.dump(saved_dict, open(file_path+'saved_dict.pkl', 'wb'))  # Dictionary with important parameters\n",
        "pickle.dump(mode_dict, open(file_path+'mode_dict.pkl', 'wb'))  #  Dictionary with most frequent values of columns"
      ],
      "metadata": {
        "id": "WqSwjRej03lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Onehot encoder for categorical columns\n",
        "pickle.dump(ohe_proto, open(file_path+'ohe_proto.pkl', 'wb'))\n",
        "pickle.dump(ohe_service, open(file_path+'ohe_service.pkl', 'wb'))\n",
        "pickle.dump(ohe_state, open(file_path+'ohe_state.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "WgByUbL505Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaned and processed train data\n",
        "pickle.dump((x_train, y_train), open(file_path+'final_train.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "3yvllXMw06e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline functions\n",
        "\n",
        "We have to prepare a pipeline, where we can send raw data and get the output.\n",
        "\n",
        "We will use test data to implement the pipeline. Here we will use all the parameters we have saved using train data.\n",
        "\n",
        "Also standardize and onehot encode test data using train data objects for standardscaler and onehotencoder."
      ],
      "metadata": {
        "id": "vvqV1zTx0_in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "    '''\n",
        "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
        "    Check for columns datatype and fix them.\n",
        "    '''\n",
        "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
        "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
        "\n",
        "    # Cleaning the data\n",
        "    for col in data.columns:\n",
        "        val = mode_dict[col]  # Mode value of the column in train data\n",
        "        data[col] = data[col].fillna(value=val)\n",
        "        data[col] = data[col].replace(' ', value=val)\n",
        "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
        "\n",
        "        # Fixing binary columns\n",
        "        if col in saved_dict['binary_col']:\n",
        "            data[col] = np.where(data[col]>1, val, data[col])\n",
        "\n",
        "    # Fixing datatype of columns\n",
        "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
        "    for bad_col in bad_dtypes:\n",
        "        data[col] = data[col].astype(float)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "F8mKM0za1EHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_log1p(data):\n",
        "    '''\n",
        "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
        "    '''\n",
        "    for col in saved_dict['log1p_col']:\n",
        "        new_col = col + '_log1p'  # New col name\n",
        "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
        "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
        "    return data"
      ],
      "metadata": {
        "id": "K01y8gwH1F8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(data):\n",
        "    '''\n",
        "    Stanardize the given data. Performs mean centering and varience scaling.\n",
        "    Using stanardscaler object trained on train data.\n",
        "    '''\n",
        "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
        "    return data"
      ],
      "metadata": {
        "id": "g4wT8iUP1HfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ohencoding(data):\n",
        "    '''\n",
        "    Onehot encoding the categoricla columns.\n",
        "    Add the ohe columns with the data and removes categorical columns.\n",
        "    Using Onehotencoder objects trained on train data.\n",
        "    '''\n",
        "\n",
        "    # Onehot encoding cat col using onehotencoder objects\n",
        "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
        "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
        "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
        "\n",
        "    # Adding encoding data to original data\n",
        "    data = pd.concat([data,\n",
        "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
        "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
        "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
        "                      axis=1)\n",
        "\n",
        "    # Removing cat columns\n",
        "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "gqtAqSsd1I_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading all the objects from disk, that we have trained on train data."
      ],
      "metadata": {
        "id": "N6AxTqDc1Mw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametrs\n",
        "saved_dict = pickle.load(open(file_path+'saved_dict.pkl', 'rb'))\n",
        "# Mode value of all the columns\n",
        "mode_dict = pickle.load(open(file_path+'mode_dict.pkl', 'rb'))\n",
        "# Stanardscaler object\n",
        "scaler = pickle.load(open(file_path+'scaler.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "Q9lm0_1X1KzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoder objects\n",
        "ohe_proto = pickle.load(open(file_path+'ohe_proto.pkl', 'rb'))\n",
        "ohe_service = pickle.load(open(file_path+'ohe_service.pkl', 'rb'))\n",
        "ohe_state = pickle.load(open(file_path+'ohe_state.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "dpObfqJw1PGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "JACuWDMQ1QwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting index of test data\n",
        "x_test.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "A3gBTFlJ1SBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "VcCmq4TY1VSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.columns"
      ],
      "metadata": {
        "id": "-rkorc151W9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding column names\n",
        "x_test.columns = saved_dict['columns']"
      ],
      "metadata": {
        "id": "ke9inKBJ1YcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new Feature\n",
        "x_test['network_bytes'] = x_test['dbytes'] + x_test['sbytes']"
      ],
      "metadata": {
        "id": "RmD6Omi-1aM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping all the unwanted columns\n",
        "dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
        "x_test.drop(columns=dropable_col, inplace=True)"
      ],
      "metadata": {
        "id": "j0rKHlZk1dSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "Q69rX1wi1fP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning data using clean_data()\n",
        "x_test = clean_data(x_test)"
      ],
      "metadata": {
        "id": "_qYWd9x91ggx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "xN1jSV4S1h3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FE: applying log1p using apply_log1p()\n",
        "x_test = apply_log1p(x_test)"
      ],
      "metadata": {
        "id": "VU2Z7Uig1jX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "mKtsbxl31k3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.columns"
      ],
      "metadata": {
        "id": "o1QmDnVP1mMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardscaling using stanardize()\n",
        "x_test = standardize(x_test)"
      ],
      "metadata": {
        "id": "C2yh2pWY1qUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "RJrKSOI71rqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Onehot encoding categorical columns using ohencoding()\n",
        "x_test = ohencoding(x_test)"
      ],
      "metadata": {
        "id": "_dKAwH491s2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "12SO7pPl1t5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test data\n",
        "x_test.head()"
      ],
      "metadata": {
        "id": "9CULUlCm1vfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matching test data columns with train data columns\n",
        "all(x_train.columns == x_test.columns)"
      ],
      "metadata": {
        "id": "MuqzF3Ai1w2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNSW-NB15: Machine Learning Models"
      ],
      "metadata": {
        "id": "lFO9tIXG910G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"./final_ipynb\""
      ],
      "metadata": {
        "id": "tXW7ONNP94AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading files"
      ],
      "metadata": {
        "id": "zmC8QK9e9_zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and Test data\n",
        "x_train, y_train = pickle.load(open(file_path+'/final_train.pkl', 'rb'))\n",
        "x_test, y_test = pickle.load(open(file_path+'/final_test.pkl', 'rb'))\n",
        "\n",
        "# Dictionaries\n",
        "saved_dict = pickle.load(open(file_path+'/saved_dict.pkl', 'rb'))\n",
        "mode_dict = pickle.load(open(file_path+'/mode_dict.pkl', 'rb'))\n",
        "\n",
        "# Standard scaler\n",
        "scaler = pickle.load(open(file_path+'/scaler.pkl', 'rb'))\n",
        "\n",
        "# Onehot encoders\n",
        "ohe_proto = pickle.load(open(file_path+'/ohe_proto.pkl', 'rb'))\n",
        "ohe_service = pickle.load(open(file_path+'/ohe_service.pkl', 'rb'))\n",
        "ohe_state = pickle.load(open(file_path+'/ohe_state.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "BkQt4mwk-A6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the train data sparse matrix\n",
        "x_train_csr = csr_matrix(x_train.values)\n",
        "\n",
        "col = x_train.columns\n",
        "\n",
        "# Creating sparse dataframe with x_train sparse matrix\n",
        "x_train = pd.DataFrame.sparse.from_spmatrix(x_train_csr, columns=col)"
      ],
      "metadata": {
        "id": "6VbrFjCi-CKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving it to disk to use later\n",
        "pickle.dump((x_train, y_train), open(file_path+'/train_sparse.pkl', 'wb'))\n",
        "\n",
        "# Loading sparse data\n",
        "x_train, y_train = pickle.load(open(file_path+'/train_sparse.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "bReUVErs-DQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.sparse.to_dense().head()"
      ],
      "metadata": {
        "id": "3_dIBLuF-Eo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "fWyhy_7G-GZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline functions"
      ],
      "metadata": {
        "id": "ALcLnzZe-IeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------------------------\n",
        "# Data Cleaning\n",
        "#------------------------------------------------------------------------------------------\n",
        "def clean_data(data):\n",
        "    '''\n",
        "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
        "    Check for columns datatype and fix them.\n",
        "    '''\n",
        "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
        "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
        "\n",
        "    # Cleaning the data\n",
        "    for col in data.columns:\n",
        "        val = mode_dict[col]  # Mode value of the column in train data\n",
        "        data[col] = data[col].fillna(value=val)\n",
        "        data[col] = data[col].replace(' ', value=val)\n",
        "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
        "\n",
        "        # Fixing binary columns\n",
        "        if col in saved_dict['binary_col']:\n",
        "            data[col] = np.where(data[col]>1, val, data[col])\n",
        "\n",
        "    # Fixing datatype of columns\n",
        "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
        "    for bad_col in bad_dtypes:\n",
        "        data[col] = data[col].astype(float)\n",
        "\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Feature Engineering: Apply log1p\n",
        "#------------------------------------------------------------------------------------------\n",
        "def apply_log1p(data):\n",
        "    '''\n",
        "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
        "    '''\n",
        "    for col in saved_dict['log1p_col']:\n",
        "        new_col = col + '_log1p'  # New col name\n",
        "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
        "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Standardizing: Mean centering an d varience scaling\n",
        "#------------------------------------------------------------------------------------------\n",
        "def standardize(data):\n",
        "    '''\n",
        "    Stanardize the given data. Performs mean centering and varience scaling.\n",
        "    Using stanardscaler object trained on train data.\n",
        "    '''\n",
        "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Onehot encoding of categorical columns\n",
        "#------------------------------------------------------------------------------------------\n",
        "def ohencoding(data):\n",
        "    '''\n",
        "    Onehot encoding the categoricla columns.\n",
        "    Add the ohe columns with the data and removes categorical columns.\n",
        "    Using Onehotencoder objects trained on train data.\n",
        "    '''\n",
        "    # Onehot encoding cat col using onehotencoder objects\n",
        "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
        "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
        "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
        "\n",
        "    # Adding encoding data to original data\n",
        "    data = pd.concat([data,\n",
        "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
        "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
        "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
        "                      axis=1)\n",
        "\n",
        "    # Removing cat columns\n",
        "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "HUkirr05-KZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_data(data, saved_dict=saved_dict, mode_dict=mode_dict):\n",
        "    '''\n",
        "    This functions takes raw input and convert that to model required output.\n",
        "    '''\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data.columns = saved_dict['columns']\n",
        "\n",
        "    data['network_bytes'] = data['dbytes'] + data['sbytes']\n",
        "\n",
        "    dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
        "    data.drop(columns=dropable_col, inplace=True)\n",
        "\n",
        "    data = clean_data(data)\n",
        "    data = apply_log1p(data)\n",
        "    data = standardize(data)\n",
        "    data = ohencoding(data)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "yEcJTUMu-Nrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation of models"
      ],
      "metadata": {
        "id": "NxuUfXxb-P5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pipeline to prepare test data\n",
        "x_test = get_final_data(x_test)"
      ],
      "metadata": {
        "id": "XeVWSVDR-PbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making test data sparse matrix\n",
        "x_test_csr = csr_matrix(x_test.values)\n",
        "col = x_test.columns\n",
        "\n",
        "# Creating x_test sparse dataframe\n",
        "x_test = pd.DataFrame.sparse.from_spmatrix(x_test_csr, columns=col)"
      ],
      "metadata": {
        "id": "nZoZ9gaS-Rwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "lWo0bMG0-Tfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "ZzuWhXAA-Vdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all(x_train.columns == x_test.columns)"
      ],
      "metadata": {
        "id": "xXvaOLV--WgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility functions"
      ],
      "metadata": {
        "id": "eh1JHCBs-X5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def false_alarm_rate(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    This function will return False Alarm Rate for given true and predicted values.\n",
        "    False Alarm Rate is average of False Negetive Rate and False Positive Rate\n",
        "    \"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    FPR = fp / (fp + tn)  # False positive rate\n",
        "    FNR = fn / (fn + tp)  # False negetive rate\n",
        "    return (FPR+FNR)/2  # False alarm rate"
      ],
      "metadata": {
        "id": "yXcSJJOR-Zuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hyperparam_tuning(clf, parameters, x, y, method='gridsearch', cv=None):\n",
        "    \"\"\"\n",
        "    Utility function for Tuning hyperparameters\n",
        "    \"\"\"\n",
        "    # initialization\n",
        "    scoring = {'auc':'roc_auc',\n",
        "               'f1':'f1',\n",
        "               'FAR':make_scorer(false_alarm_rate, greater_is_better=False)}  # using custom scoring function FAR.\n",
        "\n",
        "    # param tuning\n",
        "    if method=='gridsearch':\n",
        "        # As we are using multi scoring, So we need to point refit to scoring function with which we want to evaluate score\n",
        "        tuning_clf = GridSearchCV(clf, parameters, scoring=scoring, refit='auc',\n",
        "                                  cv=cv, verbose=3, return_train_score=True)\n",
        "        result = tuning_clf.fit(x, y)\n",
        "\n",
        "    elif method=='randomsearch':\n",
        "        tuning_clf = RandomizedSearchCV(clf, parameters, scoring=scoring, refit='auc',\n",
        "                                        cv=cv, verbose=3, return_train_score=True)\n",
        "        result = tuning_clf.fit(x, y)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "IFWRXdPA-aW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result_visualization(tuned_clf, parameters, param1=None, param2=None):\n",
        "    \"\"\"\n",
        "    Utility function to plot the results obtained after hyper parameter tuning\n",
        "    \"\"\"\n",
        "    # getting train and test scores to plot\n",
        "    train_score = tuned_clf.cv_results_['mean_train_auc']\n",
        "    test_score = tuned_clf.cv_results_['mean_test_auc']\n",
        "\n",
        "    # print results\n",
        "    print(\"Best Score: {}\".format(tuned_clf.best_score_))\n",
        "    print(\"Best Parameters: {}\".format(tuned_clf.best_params_))\n",
        "\n",
        "    # Want to visualize score with 2 params, plot heatmap of the 3 variables\n",
        "    if param2 is not None:\n",
        "        # heatmap\n",
        "        train_cmap=sns.light_palette(\"green\")\n",
        "        test_cmap=sns.light_palette(\"blue\")\n",
        "\n",
        "        # Figure and axis\n",
        "        fig = plt.figure(figsize=(20,5))\n",
        "        train_ax = fig.add_subplot(1,2,1)\n",
        "        test_ax = fig.add_subplot(1,2,2)\n",
        "\n",
        "        # train heatmap\n",
        "        sns.heatmap(train_score.reshape(len(parameters[param1]), len(parameters[param2])),\n",
        "                    cmap=train_cmap, annot=True, fmt='.4f', ax=train_ax,\n",
        "                    xticklabels=parameters[param2], yticklabels=parameters[param1])\n",
        "\n",
        "        # test heapmap\n",
        "        sns.heatmap(test_score.reshape(len(parameters[param1]), len(parameters[param2])),\n",
        "                    cmap=test_cmap, annot=True, fmt='.6f', ax=test_ax,\n",
        "                    xticklabels=parameters[param2], yticklabels=parameters[param1])\n",
        "\n",
        "        # axis labels and plot title\n",
        "        train_ax.set_title(\"Train hyperparam heatmap\")\n",
        "        train_ax.set_xlabel(param2)\n",
        "        train_ax.set_ylabel(param1)\n",
        "\n",
        "        test_ax.set_title(\"Test hyperparam heatmap\")\n",
        "        test_ax.set_xlabel(param2)\n",
        "        test_ax.set_ylabel(param1)\n",
        "        plt.show()\n",
        "\n",
        "    # Want to visualize score with one param, simply plot score and param\n",
        "    else:\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot(np.log10(parameters[param1]), train_score, label=\"train\")\n",
        "        plt.plot(np.log10(parameters[param1]), test_score, label=\"test\")\n",
        "\n",
        "        # Plot for train data\n",
        "        for i, txt in enumerate(train_score):\n",
        "            plt.annotate((parameters[param1][i], np.round(txt, 3)), (np.log10(parameters[param1])[i], train_score[i]))\n",
        "\n",
        "        # Plot for test data\n",
        "        for i, txt in enumerate(test_score):\n",
        "            plt.annotate((parameters[param1][i], np.round(txt, 3)), (np.log10(parameters[param1])[i], test_score[i]))\n",
        "\n",
        "        plt.xlabel(param1)\n",
        "        plt.ylabel('performance')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Lk7RY7D4-c5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_result(clf, x_train, y_train, x_test, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Utility function to get result on test data from best found classifier using hyper parameter tuning.\n",
        "    Print the accuracy and False Alarm Rate for both train and test.\n",
        "    Plot confusion matrix of train and test.\n",
        "    \"\"\"\n",
        "    cmap=sns.light_palette(\"blue\")\n",
        "    labels= ['non-attack', 'attack']\n",
        "\n",
        "    # Training te best model\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_train_pred = clf.predict(x_train)  # prediction on train data\n",
        "    y_test_pred = clf.predict(x_test)  # prediction on test data\n",
        "\n",
        "    # auc curve\n",
        "    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
        "    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
        "\n",
        "    # Scores of train dataset\n",
        "    train_auc = auc(train_fpr, train_tpr)\n",
        "    train_f1 = f1_score(y_train, y_train_pred)\n",
        "    train_far = false_alarm_rate(y_train, y_train_pred)\n",
        "\n",
        "    # Scores of test dataset\n",
        "    test_auc = auc(test_fpr, test_tpr)\n",
        "    test_f1 = f1_score(y_test, y_test_pred)\n",
        "    test_far = false_alarm_rate(y_test, y_test_pred)\n",
        "\n",
        "    # Printing the result as a table\n",
        "    x = PrettyTable()\n",
        "    x.field_names = ['Dataset', 'Model', 'AUC', 'F1-score', 'False Alarm Rate']\n",
        "    x.add_row(['Train', model_name, train_auc, train_f1, train_far])\n",
        "    x.add_row(['Test', model_name, test_auc, test_f1, test_far])\n",
        "    print(x)\n",
        "\n",
        "    # Plot AUC curve\n",
        "    plt.figure()\n",
        "    plt.plot(train_fpr, train_tpr, label=f\"Train AUC: {train_auc}\")\n",
        "    plt.plot(test_fpr, test_tpr, label=f\"Test AUC: {test_auc}\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion martix of train and test\n",
        "    # Train confusion matrix\n",
        "    plt.figure(figsize=(16,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.heatmap(confusion_matrix(y_train, y_train_pred),\n",
        "                annot=True, cmap=cmap, fmt='d',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Train Confusion Matrix\")\n",
        "\n",
        "    # Test confusion matrix\n",
        "    plt.subplot(1,2,2)\n",
        "    sns.heatmap(confusion_matrix(y_test, y_test_pred),\n",
        "                annot=True, cmap=cmap, fmt='d',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Test Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Returning scores of test data\n",
        "    return clf, test_auc, test_f1, test_far\n"
      ],
      "metadata": {
        "id": "RD9_x0Uq-ejz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Logistic Regression\n",
        "\n",
        "We are starting with basic Logistic Regression for training our data.\n",
        "\n",
        "Going tune hyperparameter \"alpha\" and \"penalty\""
      ],
      "metadata": {
        "id": "JLg3s50e-oHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning \"alpha\" for l2 \"penalty\"\n",
        "clf = SGDClassifier(loss='log', penalty='l2')\n",
        "param = {'alpha':np.logspace(-6, 1, 8)}  # Values of alpha\n",
        "\n",
        "lr_clf = hyperparam_tuning(clf, param, x_train_csr, y_train, cv=3)"
      ],
      "metadata": {
        "id": "qen8oeJ6-rSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the result to select best parameter\n",
        "result_visualization(lr_clf, param, param1='alpha')"
      ],
      "metadata": {
        "id": "Xlen2hpV-teM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning \"alpha\" for l1 \"penalty\"\n",
        "clf = SGDClassifier(loss='log', penalty='l1')\n",
        "param = {'alpha':np.logspace(-6, 1, 8)}  # Values of alpha\n",
        "\n",
        "lr_clf = hyperparam_tuning(clf, param, x_train_csr, y_train)"
      ],
      "metadata": {
        "id": "Ncxulzq1-u7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting alpha vs score\n",
        "result_visualization(lr_clf, param, 'alpha')"
      ],
      "metadata": {
        "id": "ErnxnDaa-wSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best LR model\n",
        "lr_bst_clf = SGDClassifier(penalty='l1', alpha=1e-6)"
      ],
      "metadata": {
        "id": "_7bo9H5c-zGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting best model's performance on train and test data\n",
        "lr_clf, lr_auc, lr_f1, lr_far = evaluate_result(lr_bst_clf, x_train, y_train, x_test, y_test, 'LR')"
      ],
      "metadata": {
        "id": "Ty6Q4FsN-1LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "1. Using wide range of values for \"alpha\" from 10^-6 to 10^3 and \"penalty\" l1 and l2\n",
        "2. Can observe that upto alpha=0.1 model's score is good and then its falling down heavily.\n",
        "3. The best parameter of the model for our data is \"penalty\" l1 and \"alpha\" 10^-6\n",
        "4. Both train and test auc score are very close that means model is not overfitting.\n",
        "5. Model is performing well but there are few False Positive points in the result"
      ],
      "metadata": {
        "id": "sjq6VLAV-33o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An empty dictionary to store the result of all the models\n",
        "result_dict = {\"name\":[], \"auc\":[], \"f1\":[], \"far\":[]}"
      ],
      "metadata": {
        "id": "JrZnwb2j-4Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storinig result for LR\n",
        "result_dict['name'].append(\"LR\")\n",
        "result_dict['auc'].append(lr_auc)\n",
        "result_dict['f1'].append(lr_f1)\n",
        "result_dict['far'].append(lr_far)"
      ],
      "metadata": {
        "id": "fqai7qCh-6tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(lr_clf, open(file_path+'/lr_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "LGnyqbKi-8nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict"
      ],
      "metadata": {
        "id": "6mCx7pWx--Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear SVC\n",
        "The next model we are going to try is Linear Support Vector Classifier\n",
        "\n",
        "Tune parameters for \"alpha\" and \"penalty\"\n",
        "\n"
      ],
      "metadata": {
        "id": "iiVT87py_DdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning \"alpha\" and \"penalty\"\n",
        "clf = SGDClassifier(loss='hinge')\n",
        "param = {'alpha':[10**x for x in range(-5,3)],  # Values for alpha\n",
        "         'penalty':['l1', 'l2']}  # l1 an d l2 penalty\n",
        "\n",
        "svm_clf = hyperparam_tuning(clf, param, x_train_csr, y_train, cv=3)"
      ],
      "metadata": {
        "id": "pgnftWJu_Gvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualising scores, using heatmap to check the performance on \"alpha\" and \"penalty\"\n",
        "result_visualization(svm_clf, param, 'alpha', 'penalty')"
      ],
      "metadata": {
        "id": "dWwcnUY1_H2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best svc model\n",
        "best_svm_clf = svm_clf.best_estimator_"
      ],
      "metadata": {
        "id": "DKXfOIV-_JvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model performance on train and test\n",
        "svm_clf, svm_auc, svm_f1, svm_far = evaluate_result(clf, x_train_csr, y_train, x_test_csr, y_test, \"SVM\")"
      ],
      "metadata": {
        "id": "nwZuKY7p_MTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "1. Using wide range of \"alpha\" values from 10^-5 to 10^2 and l1, l2 \"penalty\"\n",
        "2. For penalty=l1 model is behaving like a random model with auc=0.5 from alpha=1 to 100.\n",
        "3. Best set of parameters for the model is alpha=10^-4 and penalty=l2\n",
        "4. Train and Test score is very close so no overfitting here, getting better value of auc and FAR tha Logistic Regression\n",
        "5. FAR value of this model is really good. The no of FP increased than LR but there are very few FN points in the result."
      ],
      "metadata": {
        "id": "1CWb5Rqv_Pyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving result of SVM\n",
        "result_dict['name'].append(\"SVM\")\n",
        "result_dict['auc'].append(svm_auc)\n",
        "result_dict['f1'].append(svm_f1)\n",
        "result_dict['far'].append(svm_far)"
      ],
      "metadata": {
        "id": "4zKD6Kld_Oyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(svm_clf, open(file_path+'/svm_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "4IRf_QFJ_O2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(result_dict, open('result_dict.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "dqPUuMDS_VVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree\n",
        "Trying out DT for the next model.\n",
        "\n",
        "Tune parameters for \"max_depth\", \"min_samples_split\" and \"min_samples_leaf\""
      ],
      "metadata": {
        "id": "oSXGMj9n_YQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DT classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "param = {'max_depth':[8, 10, 12, 14],\n",
        "         'min_samples_split':[2, 4, 6]}\n",
        "\n",
        "dt_clf = hyperparam_tuning(clf, param, x_train_csr, y_train, cv=3)"
      ],
      "metadata": {
        "id": "nGQsu_zf_ayH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting heatmap of scores with params\n",
        "result_visualization(dt_clf, param, 'max_depth', 'min_samples_split')"
      ],
      "metadata": {
        "id": "0Pq1mMMb_evA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf.best_estimator_"
      ],
      "metadata": {
        "id": "uP5MO4Jb_gQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning \"min_samples_leaf\" on top of best found params\n",
        "clf = dt_clf.best_estimator_\n",
        "param = {'min_samples_leaf':[9, 11, 13]}\n",
        "\n",
        "dt_clf = hyperparam_tuning(clf, param, x_train_csr, y_train,cv=3)"
      ],
      "metadata": {
        "id": "pgxVuoHD_heq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_visualization(dt_clf, param, 'min_samples_leaf')"
      ],
      "metadata": {
        "id": "hKeIjrh7_jmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best value of \"min_samples_leaf\" is 11"
      ],
      "metadata": {
        "id": "22YlaOw8_lSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_param = {'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf':9}\n",
        "\n",
        "dt_best_clf = DecisionTreeClassifier(**dt_param)"
      ],
      "metadata": {
        "id": "_0PLaAb0_lwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf, dt_auc, dt_f1, dt_far = evaluate_result(dt_best_clf, x_train_csr, y_train, x_test_csr, y_test, 'DT')"
      ],
      "metadata": {
        "id": "AoXopiEB_nv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Model to disk\n",
        "pickle.dump(dt_clf, open(file_path+'/dt_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "sk3v6Ldi_p20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = pickle.load(open('./result_dict.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "QMgfs5VI_rq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving scores of DT\n",
        "result_dict['name'].append(\"DT\")\n",
        "result_dict['auc'].append(dt_auc)\n",
        "result_dict['f1'].append(dt_f1)\n",
        "result_dict['far'].append(dt_far)"
      ],
      "metadata": {
        "id": "8DIPae-0_stH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict"
      ],
      "metadata": {
        "id": "qhFKL_qM_uSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "1. Tuning parameters for \"max_depth\", \"min_sampples_split\" and \"min_samples_leaf\".\n",
        "2. Performance is mostly dependent on \"max_depth\" and less dependent on other 2 params.\n",
        "3. Best parameters for the model max_depth=10, min_samples_split=6 and min_samples_leaf=9\n",
        "4. Train and Test score is very close so no overfitting here, getting better value of F1 score with this model than the other 2 abve.\n",
        "5. NO of False Positive reduced for this model."
      ],
      "metadata": {
        "id": "g5SJEb5O_vcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation function\n",
        "\n",
        "For Random Forest and XGBClassifier, we will tuning its hyperparameters with train and cv data and using loops. Because the dataset is very large and with grid or randomsearch it will take so much time."
      ],
      "metadata": {
        "id": "IAR9-w3N_yRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train in train and cv data\n",
        "x_train_new_csr, x_cv_csr, y_train, y_cv = train_test_split(x_train_csr, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CaBLqWLS_0km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_new_csr.shape, y_train.shape, x_cv_csr.shape, y_cv.shape, x_test_csr.shape, y_test.shape"
      ],
      "metadata": {
        "id": "vN1mdWw__2JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(clf_name, param, param_tune,\n",
        "                     x_train=x_train_new_csr, y_train=y_train, x_cv=x_cv_csr, y_cv=y_cv):\n",
        "    \"\"\"\n",
        "    Using this function to cross validate with train and cv data.\n",
        "    Parameters:\n",
        "        clf_name = Name of the classifier\n",
        "        param = Dictionary of hyperparameters and params for classifier\n",
        "        param_tune = Name of the parameter to tune\n",
        "        x_train, y_train = Train data and Train label\n",
        "        x_cv, y_cv = CV data and CV label\n",
        "    \"\"\"\n",
        "    # Initializing empty list to store train and cv scores\n",
        "    train_auc_list = []\n",
        "    cv_auc_list = []\n",
        "\n",
        "    # Assigning arguments to variables\n",
        "    hyper_param = param_tune\n",
        "    values = param.pop(hyper_param)\n",
        "    classifier = clf_name\n",
        "\n",
        "    # Looping through the list of hyper parameter\n",
        "    for i in tqdm(values):\n",
        "        clf = classifier(**{hyper_param:i}, **param, n_jobs=-1)\n",
        "        clf.fit(x_train, y_train)  # Training\n",
        "\n",
        "        # Predicting probability of targets\n",
        "        y_train_pred = clf.predict_proba(x_train)[:,1]\n",
        "        y_cv_pred = clf.predict_proba(x_cv)[:,1]\n",
        "\n",
        "        # Getting scores\n",
        "        train_auc_list.append(roc_auc_score(y_train, y_train_pred))\n",
        "        cv_auc_list.append(roc_auc_score(y_cv, y_cv_pred))\n",
        "\n",
        "        print(\"For {}: {}, train auc: {} and test auc {}\".format(hyper_param, i, train_auc_list[-1], cv_auc_list[-1]))\n",
        "\n",
        "    # Plotting scores\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(values, train_auc_list, label=\"train\")\n",
        "    plt.plot(values, cv_auc_list, label=\"cv\")\n",
        "\n",
        "    for i, txt in enumerate(train_auc_list):\n",
        "        plt.annotate((values[i], np.round(txt, 3)), (values[i], train_auc_list[i]))\n",
        "    for i, txt in enumerate(cv_auc_list):\n",
        "        plt.annotate((values[i], np.round(txt, 3)), (values[i], cv_auc_list[i]))\n",
        "\n",
        "    plt.xlabel(hyper_param)\n",
        "    plt.ylabel('auc score')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Finding best parameter with highest cv score\n",
        "    best_param = np.argmax(cv_auc_list)\n",
        "    clf = classifier(**{hyper_param:values[best_param]}, **param)  # Trained model\n",
        "\n",
        "    return clf, hyper_param, values[best_param]"
      ],
      "metadata": {
        "id": "Lst5UtTf_5Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest\n",
        "\n",
        "Trying Random Forest Classifier next.\n",
        "<br>\n",
        "Tuning 'n_estimators', 'max_depth', 'min_samples_split' and 'criterion'"
      ],
      "metadata": {
        "id": "t5M9-Qbb_8do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier Name\n",
        "CLF_NAME = RandomForestClassifier"
      ],
      "metadata": {
        "id": "d7JIzVYZ_9vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning No of estimators\n",
        "param = {'n_estimators':[100, 200, 300, 400]}\n",
        "rf_clf, param1, val1 = cross_validation(CLF_NAME, param, 'n_estimators')"
      ],
      "metadata": {
        "id": "ddkQiozZ_-ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning Max depth\n",
        "param = {'max_depth':[20, 22, 24]}\n",
        "dt_clf, param2, val2 = cross_validation(CLF_NAME, param, 'max_depth')"
      ],
      "metadata": {
        "id": "GtMGa8UGAAIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning Min samples split\n",
        "param = {'min_samples_split':[2, 4, 6], 'max_depth':22}\n",
        "dt_clf = cross_validation(CLF_NAME, param, 'min_samples_split')"
      ],
      "metadata": {
        "id": "WbmudCpbABxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuning Criterion\n",
        "param = {'criterion':['gini', 'entropy'], 'min_samples_split':6, 'max_depth':22}\n",
        "dt_clf, _, _ = cross_validation(CLF_NAME, param, 'criterion')"
      ],
      "metadata": {
        "id": "mHmuYtMRADBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best RandomForest model\n",
        "rf_bst_clf = RandomForestClassifier(criterion='gini', max_depth=22, min_samples_split=6, n_estimators=300, n_jobs=-1)"
      ],
      "metadata": {
        "id": "45FZdpOaAEb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting result on train and test data\n",
        "evaluate_result(rf_bst_clf, x_train_new_csr, y_train, x_test_csr, y_test, \"RF\")"
      ],
      "metadata": {
        "id": "hvktK6qbAFlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "1. Tuning lots of parameters for this classifier with appropriate values.\n",
        "2. Performance is mostly dependent on \"n_estimators\", \"max_depth\" and less dependent on other 2 params.\n",
        "3. Best parameters for the model: criterion='gini', max_depth=22, min_samples_split=6, n_estimators=300\n",
        "4. Train and Test score is  close but compared to above models there is a gap between train and test score. So it is overfitting on train data if wwe compare with above models. But the gap is very low so not much of overfitting.\n",
        "5. No of False Positive reduced heavily, but No of False Negetive incresed."
      ],
      "metadata": {
        "id": "pppGzvBYAH8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(rf_bst_clf, open(file_path+'/rf_best_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "X68nlybPAIq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = pickle.load(open('result_dict.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "D2pVT0hvALnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding results\n",
        "result_dict['name'].append(\"RF\")\n",
        "result_dict['auc'].append(0.9854768258366028)\n",
        "result_dict['f1'].append(0.9767504956694146)\n",
        "result_dict['far'].append(0.01452317416339732)"
      ],
      "metadata": {
        "id": "3SqBb-5IAMlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_dict)"
      ],
      "metadata": {
        "id": "A0ihppmLANfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(result_dict, open(file_path+'/final_result_dict.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "XdztIdufAOST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GBDT\n",
        "\n",
        "Trying out Gradient boosted decision tree.\n",
        "\n",
        "For this model there are lots of hyperparameters to tune like, 'learning_rate', 'max_depth', 'colsample_bylevel', 'subsample' and 'n_estimators'"
      ],
      "metadata": {
        "id": "LGviVLYyAQvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLF_NAME = xgb.XGBClassifier"
      ],
      "metadata": {
        "id": "tkUyYBNOAP1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'learning_rate':[0.001, 0.01, 0.1]}\n",
        "xgb_clf = cross_validation(CLF_NAME, param, 'learning_rate')"
      ],
      "metadata": {
        "id": "4se0KKZgATVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'max_depth':[4, 8, 12]}\n",
        "\n",
        "xgb_clf, _, _ = cross_validation(CLF_NAME, param, 'max_depth')\n"
      ],
      "metadata": {
        "id": "7sZNULMXAVoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample = np.random.randint(0, x_train.shape[0], size=700000)"
      ],
      "metadata": {
        "id": "EymfrfChAXll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'colsample_bylevel':[0.1, 0.5, 1]}\n",
        "xgb_clf,_,_ = cross_validation(CLF_NAME, param, 'colsample_bylevel', x_train=x_train.iloc[train_sample], y_train=y_train.iloc[train_sample])"
      ],
      "metadata": {
        "id": "aVCmwm0DAYqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'subsample':[0.1, 0.4, 0.7, 1]}\n",
        "xgb_xlf,_,_ = cross_validation(CLF_NAME, param, 'subsample', x_train=x_train.iloc[train_sample], y_train=y_train.iloc[train_sample])"
      ],
      "metadata": {
        "id": "Xs5zr5xFAZ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param = {'n_estimators':[200, 300, 400]}\n",
        "xgb_clf,_,_ = cross_validation(CLF_NAME, param, 'n_estimators', x_train=x_train.iloc[train_sample], y_train=y_train.iloc[train_sample])"
      ],
      "metadata": {
        "id": "ItQFJMZ7Ae-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {'n_estimators':400,\n",
        "               'max_depth':12,\n",
        "               'learning_rate':0.1,\n",
        "               'colsample_bylevel':0.5,\n",
        "               'subsample':0.1,\n",
        "               'n_jobs':-1}\n",
        "\n",
        "xgb_bst_clf = xgb.XGBClassifier(**best_params)\n",
        "xgb_clf, xgb_auc, xgb_f1, xgb_far = evaluate_result(xgb_bst_clf, x_train_new_csr, y_train, x_test_csr, y_test, 'XGB')"
      ],
      "metadata": {
        "id": "aRl6GWeiAg3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(xgb_clf, open(file_path+'/xgb_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "tj9ToX1QAjGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict = pickle.load(open(file_path+'/final_result_dict.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "aqswjro3AkRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_dict['name'].append('XGB')\n",
        "result_dict['auc'].append(xgb_auc)\n",
        "result_dict['f1'].append(xgb_f1)\n",
        "result_dict['far'].append(xgb_far)"
      ],
      "metadata": {
        "id": "IlaGO3pKAlQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_dict)[['name', 'auc', 'f1', 'far']]"
      ],
      "metadata": {
        "id": "UtTTAi6YAmgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**\n",
        "1. Tuning lots of parameters for this classifier with appropriate values.\n",
        "2. Performance is mostly dependent on \"learning_rate\" somewhat on \"max_depth\" and \"n_estimators\" and less dependent on other 2 params.\n",
        "3. Best parameters for the model- 'n_estimators':400, 'max_depth':12, 'learning_rate':0.1,              'colsample_bylevel':0.5, 'subsample':0.1,              'n_jobs':-1\n",
        "4. Train and Test score is  close but compared to above models there is a gap between train and test score. So it is overfitting on train data if wwe compare with above models. But the gap is very low so not much of overfitting.\n",
        "5. In train data FAR is very very low, but in test there is still almost equal FN and FP"
      ],
      "metadata": {
        "id": "ZK9s_9LkAoX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(result_dict, open('result_dict.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "jQxzQSdcApoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XGB_clf = pickle.load(open(file_path+'/xgb_clf.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "OmOtZAWMArC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Important features"
      ],
      "metadata": {
        "id": "nwYk6CTIH0Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Totol features with some importance\n",
        "np.count_nonzero(XGB_clf.feature_importances_)"
      ],
      "metadata": {
        "id": "fM856BBMH2r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the indices of the features sorted in descending order\n",
        "indices = np.argsort(-1 * abs(xgb_clf.feature_importances_))[:55]"
      ],
      "metadata": {
        "id": "LHE9XjGwH4Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving for later use\n",
        "saved_dict['imp_indices'] = indices"
      ],
      "metadata": {
        "id": "LtBy1OpSH5iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(saved_dict, open(file_path+'/saved_dict.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "3xUnCPr8IDN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_new_csr[:,indices].shape"
      ],
      "metadata": {
        "id": "qtaA5wBrIE99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Features and their Importance"
      ],
      "metadata": {
        "id": "nT0USjzlIGYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.bar(x_train.columns[indices], abs(xgb_clf.feature_importances_[indices]))\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Plot of features and its importance\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCRz82uqIIHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models on Important features\n",
        "\n",
        "Again Training few models from above and then compare the result of model with all parameters and only important parameters."
      ],
      "metadata": {
        "id": "bed2XKnRIMWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DT with Imp Param"
      ],
      "metadata": {
        "id": "TiCIFRfbIQkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clf_imp = DecisionTreeClassifier(max_depth=10, min_samples_split=6, min_samples_leaf=9)\n",
        "dt_clf_imp, dt_fi_auc, dt_fi_f1, dt_fi_far = evaluate_result(dt_clf_imp, x_train_new_csr[:,indices], y_train, x_test_csr[:,indices], y_test, 'DT_FI')"
      ],
      "metadata": {
        "id": "tsj_XCy8IJ8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#result_dict = pickle.load(open('result_dict.pkl', 'rb'))\n",
        "\n",
        "result_dict['name'].append('DT_FI')\n",
        "result_dict['auc'].append(dt_fi_auc)\n",
        "result_dict['f1'].append(dt_fi_f1)\n",
        "result_dict['far'].append(dt_fi_far)"
      ],
      "metadata": {
        "id": "V6Zmwk1eITZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_dict)"
      ],
      "metadata": {
        "id": "654SdsH5IWD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "1. Retraining the best Decision Tree model with data containing only important features.\n",
        "2. From observing the result we can see that there is not much difference in scores, but yes they have reduced very little."
      ],
      "metadata": {
        "id": "sPhs_wozIWpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(dt_clf_imp, open(file_path+'/dt_clf_imp.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "vbcPb3doIZE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RF with Imp Params"
      ],
      "metadata": {
        "id": "SqGGjW4mIbb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_imp = RandomForestClassifier(criterion='gini', max_depth=22, min_samples_split=6, n_estimators=300, n_jobs=-1)\n",
        "rf_clf_imp, rf_fi_auc, rf_fi_f1, rf_fi_far = evaluate_result(rf_clf_imp, x_train_new_csr[:,indices], y_train, x_test_csr[:,indices], y_test, 'RF_FI')"
      ],
      "metadata": {
        "id": "cqONEgVIIdL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(rf_clf_imp, open(file_path+'/rf_clf_imp.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "9sgxOOOFIlnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in zip(result_dict.keys(), [\"RF_FI\", rf_fi_auc, rf_fi_f1, rf_fi_far]):\n",
        "    result_dict[i].append(j)"
      ],
      "metadata": {
        "id": "hQbuR1vSImSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_dict)"
      ],
      "metadata": {
        "id": "zcsaIOK6IoZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "1. Training the best Random Forest model again with only important features\n",
        "2. We can see that the performance has increased\n",
        "3. This model is giving the highest scores amongst all the models we have trained till now"
      ],
      "metadata": {
        "id": "TMTDT1jtIqa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-creating data with imp feat\n",
        "\n",
        "x_train = x_train_new_csr[:, indices]\n",
        "x_cv = x_cv_csr[:, indices]\n",
        "x_test = x_test_csr[:, indices]"
      ],
      "metadata": {
        "id": "hnuZjXPAIr0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, x_cv.shape, x_test.shape"
      ],
      "metadata": {
        "id": "qSNd5l0RItBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembles"
      ],
      "metadata": {
        "id": "7TLa2aZjIv2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voting Classifier Model\n",
        "\n",
        "Trining a ensemble model using voting classifier, Taking 3 classifiers DecisionTree, RandomForest and XGBClassifier"
      ],
      "metadata": {
        "id": "GzrsgpqzIwrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "id": "jwr5JHvfIytP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing all the 3 classifiers with best found parameters\n",
        "dt_clf = DecisionTreeClassifier(max_depth=10, min_samples_split=6, min_samples_leaf=9)\n",
        "rf_clf = RandomForestClassifier(criterion='gini', max_depth=22, min_samples_split=6, n_estimators=300, n_jobs=-1)\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=400, max_depth=12, learning_rate=0.1, colsample_bylevel=0.5, subsample=0.1, n_jobs=-1)"
      ],
      "metadata": {
        "id": "aKe2nmMTI44D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result with voting classifier\n",
        "v_clf = VotingClassifier(estimators=[('dt', dt_clf), ('rf', rf_clf), ('xgb', xgb_clf)], voting='soft', n_jobs=-1)\n",
        "v_clf, v_auc, v_f1, v_far = evaluate_result(v_clf, x_train, y_train, x_test, y_test, \"Voting Clf\")"
      ],
      "metadata": {
        "id": "a-Z3Ym6DI5mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(v_clf, open(file_path+'/v_clf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "dDrfgRaNI9J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,j in zip(result_dict.keys(), [\"Voting\", v_auc, v_f1, v_far]):\n",
        "    result_dict[i].append(j)"
      ],
      "metadata": {
        "id": "u35CuQf7I_P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(result_dict)"
      ],
      "metadata": {
        "id": "DFV0qWk9JBof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**\n",
        "1. Training this model using 3 model best Dt, best RF and best XGB.\n",
        "2. The auc score of this model is higher than any other model.\n",
        "3. There is a gap in train and test auc and higher gap in train and test f1 and FAR\n",
        "4. In train the FAR is very very low but in test there are still few FP and FN and they are almost equal in number"
      ],
      "metadata": {
        "id": "oF3PoBCZJFw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(result_dict, open(file_path+'/final_result_dict.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "Kaw1gAjCJGj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = pickle.load(open(file_path+'/rf_clf_imp.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "gzKtSnC2JIAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test, best_model.predict(x_test_csr[:, indices]))"
      ],
      "metadata": {
        "id": "9qEkLegCJJ9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Notebook"
      ],
      "metadata": {
        "id": "mwmRvIQHKfhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to load files\n",
        "file_path = \"./final_ipynb/\""
      ],
      "metadata": {
        "id": "bV1cB0bHKiZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "jy6he5-ZKkvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw data and labels\n",
        "x_test, y_test = pickle.load(open(file_path+'final_test.pkl', 'rb'))\n",
        "\n",
        "# Dictionary with useful parameters\n",
        "saved_dict = pickle.load(open(file_path+'saved_dict.pkl', 'rb'))\n",
        "# Dictionary with mode of all the columns of train data, useful for fill any values in test\n",
        "mode_dict = pickle.load(open(file_path+'mode_dict.pkl', 'rb'))\n",
        "\n",
        "# Standardscaler\n",
        "scaler = pickle.load(open(file_path+'scaler.pkl', 'rb'))\n",
        "\n",
        "# Ohehotencoders\n",
        "ohe_proto = pickle.load(open(file_path+'ohe_proto.pkl', 'rb'))\n",
        "ohe_service = pickle.load(open(file_path+'ohe_service.pkl', 'rb'))\n",
        "ohe_state = pickle.load(open(file_path+'ohe_state.pkl', 'rb'))\n",
        "\n",
        "# Best model found on train data, here it is Randomforest Classifier trained on only important features\n",
        "best_model = pickle.load(open(file_path+'rf_clf_imp.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "A1b0lbkRKld_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "D0oZsUF-KmyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------------------------------------------------\n",
        "# Data Cleaning\n",
        "#------------------------------------------------------------------------------------------\n",
        "def clean_data(data):\n",
        "    '''\n",
        "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
        "    Check for columns datatype and fix them.\n",
        "    '''\n",
        "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
        "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
        "\n",
        "    # Cleaning the data\n",
        "    for col in data.columns:\n",
        "        val = mode_dict[col]  # Mode value of the column in train data\n",
        "        data[col] = data[col].fillna(value=val)\n",
        "        data[col] = data[col].replace(' ', value=val)\n",
        "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
        "        # Fixing binary columns\n",
        "        if col in saved_dict['binary_col']:\n",
        "            data[col] = np.where(data[col]>1, val, data[col])\n",
        "\n",
        "    # Fixing datatype of columns\n",
        "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
        "    for bad_col in bad_dtypes:\n",
        "        data[col] = data[col].astype(float)\n",
        "\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Feature Engineering: Apply log1p\n",
        "#------------------------------------------------------------------------------------------\n",
        "def apply_log1p(data):\n",
        "    '''\n",
        "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
        "    '''\n",
        "    for col in saved_dict['log1p_col']:\n",
        "        new_col = col + '_log1p'  # New col name\n",
        "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
        "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Standardizing: Mean centering an d varience scaling\n",
        "#------------------------------------------------------------------------------------------\n",
        "def standardize(data):\n",
        "    '''\n",
        "    Stanardize the given data. Performs mean centering and varience scaling.\n",
        "    Using stanardscaler object trained on train data.\n",
        "    '''\n",
        "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Onehot encoding of categorical columns\n",
        "#------------------------------------------------------------------------------------------\n",
        "def ohencoding(data):\n",
        "    '''\n",
        "    Onehot encoding the categoricla columns.\n",
        "    Add the ohe columns with the data and removes categorical columns.\n",
        "    Using Onehotencoder objects trained on train data.\n",
        "    '''\n",
        "    # Onehot encoding cat col using onehotencoder objects\n",
        "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
        "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
        "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
        "\n",
        "    # Adding encoding data to original data\n",
        "    data = pd.concat([data,\n",
        "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
        "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
        "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
        "                      axis=1)\n",
        "\n",
        "    # Removing cat columns\n",
        "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "DLgpyjqQKo6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_fun_1(X):\n",
        "    \"\"\"\n",
        "    This function includes entire pipeline, from data preprocessing to making final predictions.\n",
        "    It takes take in raw data as input.\n",
        "    It returns predictions for given input. Here the input can be a single point or a set of points.\n",
        "    \"\"\"\n",
        "    # Using dataframe for data preprocessing, So if single point given as input\n",
        "    # Then converting that to DataFrame with 1 row\n",
        "    if isinstance(X, pd.core.series.Series):\n",
        "        # For single input as series\n",
        "        data = pd.DataFrame(X.values.reshape(1, -1)).copy()\n",
        "    else:\n",
        "        data = X.copy()\n",
        "\n",
        "    # Resetting index of the given df and adding correct columns names mentioned in research paper\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data.columns = saved_dict['columns']\n",
        "\n",
        "    # FE: Adding 1 new feature\n",
        "    data['network_bytes'] = data['dbytes'] + data['sbytes']\n",
        "\n",
        "    # Dropping columns not needed for prediction\n",
        "    dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
        "    data.drop(columns=dropable_col, inplace=True)\n",
        "\n",
        "    # Cleaning and preprocessig\n",
        "    data = clean_data(data)\n",
        "    data = apply_log1p(data)\n",
        "    data = standardize(data)\n",
        "    data = ohencoding(data)\n",
        "\n",
        "    # Using only Important features\n",
        "    data = data.iloc[:, saved_dict['imp_indices']]\n",
        "\n",
        "    # Predicting using best model\n",
        "    predictions = best_model.predict(data)\n",
        "\n",
        "    # Returning predictions on the given data\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "DZxVKaukKrld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "pP2gSOGqKtGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For single datapoint\n",
        "y_pred = final_fun_1(x_test.iloc[0])"
      ],
      "metadata": {
        "id": "8dpnZqwzKuJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, y_test.iloc[0]"
      ],
      "metadata": {
        "id": "idXyB25-Kvc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For few datapoints\n",
        "y_pred = final_fun_1(x_test.iloc[90:100])"
      ],
      "metadata": {
        "id": "A2P1FYz6Kw-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)\n",
        "print(y_test.iloc[90:100].values)"
      ],
      "metadata": {
        "id": "ivEd2wnRKyb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For entire test data\n",
        "y_pred = final_fun_1(x_test)"
      ],
      "metadata": {
        "id": "qQCkflBEKznQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f1 score of total test data\n",
        "f1_score(y_test.values, y_pred)\n",
        "\n",
        "# same as found during training"
      ],
      "metadata": {
        "id": "pfgUCnEDK027"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "rfSJdDRFK2MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_fun_2(X, Y):\n",
        "    \"\"\"\n",
        "    This function includes entire pipeline, from data preprocessing to making final predictions.\n",
        "    It takes in raw data as input along with its target values.\n",
        "    It returns the metric value that has been selected for judging model's performance\n",
        "    Also retuns auc curve, confusion, precision and recall matrix\n",
        "    \"\"\"\n",
        "    # Getting target value and predicted value for the given data\n",
        "    y_true = Y.copy()\n",
        "    y_pred = final_fun_1(X) # By using final_fun_1 getting predicted values\n",
        "\n",
        "    # auc curve\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "\n",
        "    # Confusion, precison and recall matrix\n",
        "    C = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = C.ravel()\n",
        "    FPR = fp / (fp + tn)\n",
        "    FNR = fn / (fn + tp)\n",
        "\n",
        "    P = (C/C.sum(axis=0))\n",
        "    R =(((C.T)/(C.sum(axis=1))).T)\n",
        "\n",
        "    # Scores of test dataset\n",
        "    y_auc = auc(fpr, tpr)\n",
        "    y_f1 = f1_score(y_true, y_pred)\n",
        "    y_far = (FPR+FNR)/2  # False alarm rate\n",
        "\n",
        "    # Printing the result as a table\n",
        "    x = PrettyTable()\n",
        "    x.field_names = ['AUC', 'F1-score', 'False Alarm Rate']\n",
        "    x.add_row([y_auc, y_f1, y_far])\n",
        "    print(x)\n",
        "\n",
        "    # Plotting AUC curve\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(fpr, tpr, color='r', label=f\"AUC: {y_auc}\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting Confusion, Precision, Recall Matrix\n",
        "    labels= ['non-attack', 'attack']\n",
        "    # Confusion\n",
        "    plt.figure(figsize=(18,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    sns.heatmap(C, annot=True, cmap=\"Blues\", fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    # Precision\n",
        "    plt.subplot(1,3,2)\n",
        "    sns.heatmap(P, annot=True, cmap=\"Greens\", fmt='.3f', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Precision Matrix\")\n",
        "    # Recall\n",
        "    plt.subplot(1,3,3)\n",
        "    sns.heatmap(R, annot=True, cmap=\"BuPu\", fmt='.3f', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Recall Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Retuning performance metrices\n",
        "    return y_auc, y_f1, y_far"
      ],
      "metadata": {
        "id": "heXMbxDKK31M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using entire data\n",
        "auc, f1, far = final_fun_2(x_test, y_test.values)"
      ],
      "metadata": {
        "id": "rx0f-QHAK5l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(auc, f1, far)"
      ],
      "metadata": {
        "id": "Fmc_d_wnK7Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "GKDcnKM_K8mV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XExu7qDXt-Ol",
        "CEzmOzrvqhC2",
        "3cC6wS2_uOZS",
        "JgKpN49SzQjl",
        "lFO9tIXG910G",
        "mwmRvIQHKfhy"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}